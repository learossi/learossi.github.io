<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Convergence of M-Estimators made simple(r) | Personal Projects</title>
  <meta name="description" content="1 Convergence of M-Estimators made simple(r) | Personal Projects" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Convergence of M-Estimators made simple(r) | Personal Projects" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Convergence of M-Estimators made simple(r) | Personal Projects" />
  
  
  



<meta name="date" content="2023-04-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="preface.html"/>
<link rel="next" href="gaussian-process-mbo-for-random-forests.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="convergence-of-m-estimators-made-simpler.html"><a href="convergence-of-m-estimators-made-simpler.html"><i class="fa fa-check"></i><b>1</b> Convergence of M-Estimators made simple(r)</a>
<ul>
<li class="chapter" data-level="1.1" data-path="convergence-of-m-estimators-made-simpler.html"><a href="convergence-of-m-estimators-made-simpler.html#what-is-an-m-estimator"><i class="fa fa-check"></i><b>1.1</b> What is an M-Estimator?</a></li>
<li class="chapter" data-level="1.2" data-path="convergence-of-m-estimators-made-simpler.html"><a href="convergence-of-m-estimators-made-simpler.html#consistency"><i class="fa fa-check"></i><b>1.2</b> Consistency</a></li>
<li class="chapter" data-level="1.3" data-path="convergence-of-m-estimators-made-simpler.html"><a href="convergence-of-m-estimators-made-simpler.html#rate-of-convergence"><i class="fa fa-check"></i><b>1.3</b> Rate of Convergence</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html"><i class="fa fa-check"></i><b>2</b> Gaussian Process MBO for Random Forests</a>
<ul>
<li class="chapter" data-level="2.1" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html#overview-of-gaussian-processes"><i class="fa fa-check"></i><b>2.1</b> Overview of Gaussian Processes</a></li>
<li class="chapter" data-level="2.2" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html#application-to-random-forests"><i class="fa fa-check"></i><b>2.2</b> Application to Random Forests</a></li>
<li class="chapter" data-level="2.3" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html#data"><i class="fa fa-check"></i><b>2.3</b> Data</a></li>
<li class="chapter" data-level="2.4" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html#results"><i class="fa fa-check"></i><b>2.4</b> Results</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html#grid-search"><i class="fa fa-check"></i><b>2.4.1</b> Grid Search</a></li>
<li class="chapter" data-level="2.4.2" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html#gaussian-process-search"><i class="fa fa-check"></i><b>2.4.2</b> Gaussian Process Search</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="gaussian-process-mbo-for-random-forests.html"><a href="gaussian-process-mbo-for-random-forests.html#discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-effect-of-increased-campaign-spending-on-election-results.html"><a href="the-effect-of-increased-campaign-spending-on-election-results.html"><i class="fa fa-check"></i><b>3</b> The Effect of Increased Campaign Spending on Election Results</a>
<ul>
<li class="chapter" data-level="3.1" data-path="the-effect-of-increased-campaign-spending-on-election-results.html"><a href="the-effect-of-increased-campaign-spending-on-election-results.html#background"><i class="fa fa-check"></i><b>3.1</b> Background</a></li>
<li class="chapter" data-level="3.2" data-path="the-effect-of-increased-campaign-spending-on-election-results.html"><a href="the-effect-of-increased-campaign-spending-on-election-results.html#motivating-trends"><i class="fa fa-check"></i><b>3.2</b> Motivating Trends</a></li>
<li class="chapter" data-level="3.3" data-path="the-effect-of-increased-campaign-spending-on-election-results.html"><a href="the-effect-of-increased-campaign-spending-on-election-results.html#initial-regression-analysis"><i class="fa fa-check"></i><b>3.3</b> Initial Regression Analysis</a></li>
<li class="chapter" data-level="3.4" data-path="the-effect-of-increased-campaign-spending-on-election-results.html"><a href="the-effect-of-increased-campaign-spending-on-election-results.html#iv-regression"><i class="fa fa-check"></i><b>3.4</b> IV Regression</a></li>
<li class="chapter" data-level="3.5" data-path="the-effect-of-increased-campaign-spending-on-election-results.html"><a href="the-effect-of-increased-campaign-spending-on-election-results.html#discussion-conclusion"><i class="fa fa-check"></i><b>3.5</b> Discussion &amp; Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="neural-network-methods-for-electricity-demand-forecasting.html"><a href="neural-network-methods-for-electricity-demand-forecasting.html"><i class="fa fa-check"></i><b>4</b> Neural Network Methods for Electricity Demand Forecasting</a>
<ul>
<li class="chapter" data-level="4.1" data-path="neural-network-methods-for-electricity-demand-forecasting.html"><a href="neural-network-methods-for-electricity-demand-forecasting.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="neural-network-methods-for-electricity-demand-forecasting.html"><a href="neural-network-methods-for-electricity-demand-forecasting.html#common-neural-network-designs-for-forecasting"><i class="fa fa-check"></i><b>4.2</b> Common Neural Network Designs for Forecasting</a></li>
<li class="chapter" data-level="4.3" data-path="neural-network-methods-for-electricity-demand-forecasting.html"><a href="neural-network-methods-for-electricity-demand-forecasting.html#problem-statement-and-data"><i class="fa fa-check"></i><b>4.3</b> Problem Statement and Data</a></li>
<li class="chapter" data-level="4.4" data-path="neural-network-methods-for-electricity-demand-forecasting.html"><a href="neural-network-methods-for-electricity-demand-forecasting.html#lstm-results"><i class="fa fa-check"></i><b>4.4</b> LSTM Results</a></li>
<li class="chapter" data-level="4.5" data-path="neural-network-methods-for-electricity-demand-forecasting.html"><a href="neural-network-methods-for-electricity-demand-forecasting.html#cnn-results"><i class="fa fa-check"></i><b>4.5</b> CNN Results</a></li>
<li class="chapter" data-level="4.6" data-path="neural-network-methods-for-electricity-demand-forecasting.html"><a href="neural-network-methods-for-electricity-demand-forecasting.html#analysis"><i class="fa fa-check"></i><b>4.6</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-effect-of-seed-market-competition-on-farmers.html"><a href="the-effect-of-seed-market-competition-on-farmers.html"><i class="fa fa-check"></i><b>5</b> The Effect of Seed Market Competition on Farmers</a></li>
<li class="chapter" data-level="6" data-path="chicago-taxi-demand-estimation.html"><a href="chicago-taxi-demand-estimation.html"><i class="fa fa-check"></i><b>6</b> Chicago Taxi Demand Estimation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chicago-taxi-demand-estimation.html"><a href="chicago-taxi-demand-estimation.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="chicago-taxi-demand-estimation.html"><a href="chicago-taxi-demand-estimation.html#data-1"><i class="fa fa-check"></i><b>6.1.1</b> Data</a></li>
<li class="chapter" data-level="6.1.2" data-path="chicago-taxi-demand-estimation.html"><a href="chicago-taxi-demand-estimation.html#trends"><i class="fa fa-check"></i><b>6.1.2</b> Trends</a></li>
<li class="chapter" data-level="6.1.3" data-path="chicago-taxi-demand-estimation.html"><a href="chicago-taxi-demand-estimation.html#possible-estimation-procedures"><i class="fa fa-check"></i><b>6.1.3</b> Possible Estimation Procedures</a></li>
<li class="chapter" data-level="6.1.4" data-path="chicago-taxi-demand-estimation.html"><a href="chicago-taxi-demand-estimation.html#stl-decomposition"><i class="fa fa-check"></i><b>6.1.4</b> STL Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="chicago-taxi-demand-estimation.html"><a href="chicago-taxi-demand-estimation.html#full-data-procedures"><i class="fa fa-check"></i><b>6.2</b> Full Data Procedures</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Personal Projects</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="convergence-of-m-estimators-made-simpler" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Convergence of M-Estimators made simple(r)<a href="convergence-of-m-estimators-made-simpler.html#convergence-of-m-estimators-made-simpler" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>I’ve been reading <span class="citation">Van der Vaart (<a href="#ref-vander" role="doc-biblioref">2000</a>)</span> recently and, to be frank, I’ve found parts of it confusing. Chapter 5 deals with the properties of M and Z estimators and is the bedrock for many portions of the book in that most types of statistics can be studied in the context of these sorts of estimators (sufficient conditions for the consistency of maximum likelihood are often expressed as M estimators). Unfortunately, as is very common in statistics, the estimator properties are highly polymorphic in different types of spaces and with different constraints on the estimators themselves or on their limits. This reality necessitates a variety of theorems that I find obscures the important overriding themes about the relevant convergence theorems. Instead of simply repeating the proofs in Van der Vaart, I aim here to more clearly explain the important properties of such estimators and the implications of the proofs. Detailed explanations of the proofs can be found in the book itself since I don’t really see the point of going line-by-line again here.</p>
<div id="what-is-an-m-estimator" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> What is an M-Estimator?<a href="convergence-of-m-estimators-made-simpler.html#what-is-an-m-estimator" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>M-Estimators are important because their definition is sufficiently general to include most estimators in common use. The most important is obviously maximum likelihood (whose properties are ironically usually proved using its correspondence to method-of-moments but that’s neither here nor there). The main problem M-Estimators try to solve is as such: we have a collection of iid random variables who come from one in a family of distributions indexed by a parameter <span class="math inline">\(\theta\)</span>: find a guess for <span class="math inline">\(\theta\)</span> that converges to the population value as number of realizations of your random variables increases to infinity (the estimatory is more general than that but the previous problem is usually the one it solves).</p>
<p>M-Estimators consist of a set of iid random variables, <span class="math inline">\(X_i\)</span>, and a sequence of functions <span class="math inline">\(M_n\)</span> that defines the estimator. For each realization of n random variables, the M-Estimator is the value of <span class="math inline">\(\theta\)</span> which maximizes the function <span class="math inline">\(M_n\)</span>. The most common form of an M-estimator is as a sum of n criterion functions <span class="math inline">\(m_n\)</span>:
<span class="math display">\[
\begin{equation}
\operatorname*{argmax}_\theta M_n = \operatorname*{argmax}_\theta ( \frac{1}{N}\sum_{N} m_\theta(X_i))
\end{equation}
\]</span>
A closely related concept is the Z-estimator (my guess is that it’s Z as in “zero-estimator” or of one the usual leftover fascinations of mathematicians with the German language). Z-estimators are defined similarly except they replace the argmax with an equality condition. The corresponding form with criterion functions is:
<span class="math display">\[
\Psi_n(\theta) = \frac{1}{N}\sum_{N}\psi_\theta(X_i)=0
\]</span>
The Z-estimator is the value of <span class="math inline">\(\theta\)</span> which satisfies the equality. These two estimators are equivalent for differentiable functions where <span class="math inline">\(\psi_\theta\)</span> is just the derivative of <span class="math inline">\(m_\theta\)</span>. Even if the function is non-differentiable, similar regularity conditions can be applied to maxima and zeros respectively which allows consistency proofs for one type of estimator to be applied to the other.</p>
<p>The choice of <span class="math inline">\(m_\theta\)</span> allows the estimator to function differently depending on what you want <span class="math inline">\(\theta\)</span> to estimate. Choosing <span class="math inline">\(m_\theta\)</span> to be the log-likelihood recovers the maximum likelihood estimator. A Z-estimator for <span class="math inline">\(\sum_{i=1}^{n}(X_i - \theta)\)</span> recovers the sample mean. One can also choose more general sequences of functions <span class="math inline">\(M_n\)</span> though this necessitates a separate convergence proof. This flexibility means many types of estimators can be recast in this light which allows for simpler consistency and convergence proofs.</p>
</div>
<div id="consistency" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Consistency<a href="convergence-of-m-estimators-made-simpler.html#consistency" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the case where an M-estimator is the argmax of a sum of criterion functions, the value of <span class="math inline">\(\lim\limits_{n \to \infty} \sup_{\theta} M_n\)</span> is trivially equal to <span class="math inline">\(E_X(M({\theta_0}))\)</span> where <span class="math inline">\(\theta_0\)</span> is the true parameter that maximizes the function across the true distribution function. This follows by the law of large numbers and is essentially definitional. The following proofs assume the property that <span class="math inline">\(M_n(\theta)\)</span> converges to the maximum: more precisely, for the sequence of estimators <span class="math inline">\(\theta_n\)</span>, <span class="math inline">\(M_n(\theta_n) &gt; M(\theta_0) - o(1)\)</span> where <span class="math inline">\(o(1)\)</span> is a function that converges to 0. For more complicated <span class="math inline">\(M\)</span>s which don’t make use of sums of criterion functions, this property must be proved separately of the consistency proof which makes them much more unwieldy.</p>
<p>Rather than diving into the few consistency proofs, it might merit dicussion what exactly is it that can cause an M-estimator to not be consistent. We’ve already assumed that the output (separate of the value of <span class="math inline">\(\theta\)</span>) of the M-estimator converges to the maximum across the parameter set: however, this is not the same as consistency. To be a consistent estimator, the M-estimator has to converge to a single <span class="math inline">\(\theta\)</span> which has the desired function value. There are two separate cases where this can fail. For one, if <span class="math inline">\(M_n \rightarrow M\)</span> st <span class="math inline">\(M\)</span> has multiple equal maxima it may be that <span class="math inline">\(M\)</span> oscillates between those maxima and never settles on one to converge to even in the limit. Secondly, imagine the parameter set isn’t compact. While the maxima may occur on a defined point, it may be that the maxima occurs at infinity in the limit which corresponds to a train of maxima going on infinitely for the <span class="math inline">\(M_n\)</span> which don’t even have a convergent subsequence. Consistency proofs take on different assumptions about <span class="math inline">\(M_n\)</span> that try to control these pathological conditions in order to attain consistency.</p>
<p>The most basic of consistency proofs is Wald’s consistency proof which is commonly referred to as the “classical” approach to consistency. Here, the estimator takes the criterion function form which is assumed to be upper-semicontinuous. Generally speaking, one needs some assumption about maximum of the limit estimator being unique, but Wald doesn’t assume that. So, the classical proof doesn’t rule out oscillating solutions on a set of multiple maxima and is more of a consistency in the loose hand-waving sort of way proof. Nevertheless, is does rule out divergence to infinity sorts of pathologies by assuming that the set of parameters is either compact or is eventually bounded (this may need to be a separate proof). From there, the compactness condition allows one to build a finite subcover of the parameter space except for decreasing neighborhoods surrounding the maximizing parameters. It is then shown using the convergence assumption that the probability of the parameter n being in any of those sets approaches zero.</p>
<p>This proof is nice in that it should be familiar with anyone who has taken a real analysis course. However, the assumptions are very restrictive and hard to generalize: this is especially true of the compactness assumption. Additionally, the fact that it proves convergence to a set instead of a point is problematic in that we would like to use this to put bounds on the asymptotic convergence which we can’t do if it doesn’t technically converge.</p>
<p>The next two proofs aim to remedy these concerns by adding new (and hopefully minimal) assumptions to introduce true converge to a broader range of solutions. The easiest way to replace compactness in the parameter set is to impose regularity on the type of allowed convergence to the asymptotic M-estimator. If we assume that <span class="math inline">\(M_n\)</span> converges to <span class="math inline">\(M\)</span> uniformly and the maximum of the asymptotic estimator is strictly separated (ie there is a parameter value for which all values outside a neighborhood are less) then we have consistency in a more common sense. In this case, the strict separation assumption prevents an escape to infinity since the sequence of estimators must settle on some value eventually. The uniform convergence assumption prevents highly oscillatory behavior that would create multiple points of convergence like in the Wald proof. Specifically, the uniform convergence assumption allows us to establish that <span class="math inline">\(M_n\)</span> converges in probability to <span class="math inline">\(M\)</span> which is much stronger than the Wald result.</p>
</div>
<div id="rate-of-convergence" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Rate of Convergence<a href="convergence-of-m-estimators-made-simpler.html#rate-of-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Rate of convergence proofs are difficult since M-estimators can be any arbitrary estimator as long as it fits the convergence criteria. The criterion function formulation of M-estimators is popular as this is one area where we can guarantee a radical rate of convergence given some regularity conditions (discussed below). One might expect that the criterion function formulation has the canonical <span class="math inline">\(\sqrt n\)</span> rate of convergence given that the estimator is the sample mean of the estimator results. However, each of the estimator outputs is actually taken over the product space of all the random variable realizations. Therefore, the <span class="math inline">\(M_n\)</span>s aren’t independent and the CLT doesn’t apply. The further problem is that M-estimators need not take this criterion function form and can be more pathological.
It is easy to construct a consistent M-estimator that has an arbitrarily low convergence rate as long as one is willing to throw out an arbitrarily large amount of data. Let’s do <span class="math inline">\(e^n\)</span>. Take a given sequence of estimators with a <span class="math inline">\(\sqrt n\)</span>-law. Use this to construct a new sequence of estimators where each of the previous estimators <span class="math inline">\(M_n\)</span> is repeated <span class="math inline">\(e^n\)</span> times irregardless of the outcome of the random process. This process now as a <span class="math inline">\(e^n\sqrt n\)</span> rate of convergence (it is sufficient that the values of the new norming sequence agree with the old norming sequence on one estimator for each n-length sequence in the new estimator sequence up to a multiplicative constant which is easy to check).
The problem of identifying a convergence rate is therefore too general to provide overall heuristics for any type of estimator. However, there are useful theorems that provide polynomial rates of convergence for different conditions on the estimator sequence provided it is of the criterion function form. The following example focuses on relating bounds on the changes in the population mean to bounds on the sample mean: this type of analysis will probably be familiar to those well versed in the optimization, signal-processing, or machine-learning literature. The rate of convergence is controlled by a deterministic part and a stochastic part. If the models around the true parameter are very different from the true model (ie the true model is a sharp maxima), then convergence will be faster. This faster convergence is because it will be easy to pick out the actual maxima from the “noise” due to the inherent randomness of the sampling procedure. Dually, if the sample mean converges to the true mean quickly, that level of “noise” is lower and overall convergence is therefore faster.
A more formal statement is suppose that <span class="math inline">\(M\)</span> is of the criterion function form and has bounds <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> where <span class="math inline">\(\alpha\)</span> is essentially the bound on the convergence rate of the criterion function relative to its parameter and <span class="math inline">\(\beta\)</span> is the bound on the expected deviation of the sample mean from the population mean for the criterion function (see book for a more rigorous formulation). The rate of convergence is then <span class="math inline">\(n^\frac{1}{2\alpha-2\beta}\)</span>. This can be applied to Lipschitz functions to give the usual <span class="math inline">\(\sqrt n\)</span> rate of convergence.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-vander" class="csl-entry">
Van der Vaart, A. W. 2000. <em>Asymptotic Statistics</em>. Cambridge University Press.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="preface.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="gaussian-process-mbo-for-random-forests.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
