[["preface.html", "Personal Projects Preface", " Personal Projects 2023-04-03 Preface Hey all! These are a few portfolio projects I’ve done that are relatively quick reads just to show off what I know/can do. Here’s a quick guide the all the projects which you can navigate to using the sidebar: Gaussian Processes: A look at how efficient Gaussian Process hyperparameter search is for random forests. Campaign Finance: An attempt to estimate the effect that money raised by candidates has on campaign outcomes independent of candidate popularity or affability. Electricity Demand: A comparison of commonly recommended neural network structures for predicting short-term domestic electricity demand. Seed Market Competition: My bachelors thesis in economics which analyzes the historical effect of increasing concentration in the seed market on farmer’s outcomes. Chicago Taxi Market: Forecasting demand in the Chicago Taxi market using a variety of time-series methods. "],["gaussian-process-mbo-for-random-forests.html", "1 Gaussian Process MBO for Random Forests 1.1 Overview of Gaussian Processes 1.2 Application to Random Forests 1.3 Data 1.4 Results 1.5 Discussion", " 1 Gaussian Process MBO for Random Forests 1.1 Overview of Gaussian Processes Hyperparameter tuning is a tricky subject. Oftentimes, one simply conjures up a reasonable value from the some article about whatever architecture that one read sometime or another and plays with that until it works. This type of approach is reasonable insofar as hyperparameter tuning is very, very hard and there are a lot of answers that may look, a priori, equally reasonable while providing wildly different results in practice. Take neural networks for example. One could take a given value for the learning rate. Except that there’s a lot of literature that shows learning rate and batch size interact to shape the loss the function and one can’t set them alone. Or say you want to vary the weights of different hidden layers. Except that also affects your weight initialization strategy. And it goes on. Hyperparameter optimization is often very difficult and consensus on good decisions for any one dataset (think MNIST) is frequently the product of a multitude of separate experiments. The go-to approach when one has a relatively flat prior about the efficacy of a set of parameters within a compact set is just grid search. This does get the job done but is computationally expensive and ruinously vulnerable to the curse of dimensionality when dealing with complex or highly tunable models. All these facts make efficient automated approaches to hyperparameter tuning highly desirable. The issue here is that it is often impossible (or unclear how) to differentiate model parameters with respect to the data. Either the models employ functions which themselves are discontinuous or nondifferentiable or the parameter values are integers which means there is no reasonable interpretation of the change in objective in a neighborhood around the parameter. The lack of a derivative means that most traditional continuous optimization methods become either useless or inefficient (integer programming for example). There are a few possible solutions to this problem. One is to transform the model so that it may be differentiated. VAEs use a parametric assumption on latent space to allow differentiation. Random hinge forests are a differentiable version of the random forests I will talk about later. Obviously, differentiability is not always possible (but always imposes significant additional complexity on the model). Therefore, a more flexible parameter search option is desirable. One way to do this is to model the hyperparameters and the resulting loss as a gaussian process. Gaussian processes are any collection of random variables where any finite subset thereof are distributed jointly gaussian with an arbitrarily flexible covariance matrix. In this instance, the random variables are the objective values of the function. Gaussian processes generally assume that all variables have an unconditional mean of zero; however, they may also have a non-zero conditional mean depending on the realizations of other random variables and the relevant covariances. Estimation procedures for gaussian processes exploit this fact to build flexible models of the objective function. The main procedure is as follows: one obtains a requisite number of samples of one’s objective function with differing hyperparameter values. One then chooses a stylized covariance matrix in the form of a kernel function. Technically speaking, nothing about the definition of gaussian processes above implies they are continuous (ie they might have negative or zero covariance locally). However, continuity is a property we expect from basically all our objective functions. To ensure the estimate of the objective function satisfies this continuity requirement, covariances are assumed to a kernel function of the metric of the distance between the two functions. This assumption creates a parametric framework for the covariance which otherwise could not be estimated, imposes continuity on the function, and still allows the process to be a universal approximator given sufficient sampling. With the covariance assumption in place, estimating the value of a point in the hyperparameter domain is as simple as calculating the conditional mean of a jointly gaussian variable. Of course this is just an estimate of the actual value of the objective function and may bear only a small resemblance to reality. The procedure to find the actual minima in the hyperparameter surface is an iterative one. Given the sampled points, some nonlinear optimization is used to find an estimate of the minima of the initial objective function; BSGF is the most common method but others are obviously available. The estimated minima is then sampled and included with the other points to form the basis for a new objective. This process is then repeated until some stopping rule is reached. The primary benefit here is that the number of models trained is minimized. The tradeoff is that one must pay for the cost of whatever nonlinear optimization function one is using. Whether that is beneficial largely depends on the hyperparameter space and the model among other things. 1.2 Application to Random Forests I’m going to test this on random forests. Random forests are a bagging method that allows multiple (relatively) uncorrelated trees to vote on the class of an observation. In short, multiple decision trees are fitted to individual bootstrap estimates of the sample data. When splitting each tree at each level, features are again bootstrapped to reduce tree correlation. An observation is then given a predicted class value based on the plurality of trees. Random forests are beneficial in that there are a lot of potential hyperparameters to tune. One can adjust the number of trees, maximum length of trees, the maximum number of terminal nodes, the bagging parameters for the individual trees, the bagging parameters for the nodes, and so forth. The hyperparameter space can be high dimensional if one chooses to make it so which makes it ideal for testing out gaussian process regression. Random forests are also traditionally very flexible which make overfitting an issue. So, instead of tuning the training error as the objective function, I use the test error in n-fold cross validation as the objective function. 1.3 Data I use a dataset of wine vintages from the UC Irvine machine learning repository. It has approximately 180 wines from 3 different italian wineries. Each of them were given chemical composition tests that give values for a variety of quantities like alcohol percent, hue, or the amount of phenols. There are 13 variables in total and which are split roughly equally between three possible classes. plot_data = melt(select(wine_data, !c(proline, origin)), values.name = &quot;Value&quot;) %&gt;% mutate(value = log(value)) ## No id variables; using all as measure variables ggplot(data = plot_data, aes(variable, value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(title = &quot;Variable Distributions&quot;, x = &quot;Predictive Variables&quot;, y = &quot;log(Value)&quot;) I take the log for this graph to make each level visible on the same graph but don’t do so for the data itself. Anyways, most variables have roughly the same variance, lack distinguishable tails, and have roughly similar levels. It’s nice data where I don’t have to do much (databases are so nice). The amount of data is relatively small but that’s good for me because it is still able to be used while not requiring me to run grid search on a massive dataset. The implications thereof will be discussed later. 1.4 Results 1.4.1 Grid Search I use the RandomForest package from R to do this analysis. It has a wide variety of parameters over which I can optimize and I select a few. The first is the number of trees that the random forest produces. This parameter controls the flexibility of the model; the more trees there are and the more bagged samples the trees are being trained on, the more specific models will be able to specialize on certain parts of the data. The next variable is the number of bagged variables that are sampled at each split; a lower number increases tree decorrelation but raises the number of trees required to represent the sample because each tree is less powerful. There is also a binary variable that controls if sampling with replacement is done that I allow to be toggled along with a variable that selects the sample size for each tree. And finally, the node size parameter controls how many samples can be contained within a node without it splitting. Together, these form the basis of a parameter set in the ParamHelpers pack that we can plug into the mlrMBO package to optimize it. The main metric I’m interested in is how many iterations it takes the gaussian process to obtain a solution with a comparable objective value as the one that the grid search obtains. Given that I’m concerned about speed, I could hypothetically time each method and report those speeds. However, that seems besides the point to me. As mentioned, the main factor affecting speed is the time it takes to train the model and the convergence rate of the optimization algorithm. If I found that grid search was faster, I could replace this model with a similar one that takes 10x longer to train and get the opposite result. The convergence speed of the gaussian process is something that is less subject to changes in the previous two variables and thus more interesting an object of study. Anyways, cross validation libraries in R tend to be a bit library dependent and it’s pretty easy to implement, so I write my own k-fold cross validation script. Instead of explicitly defining parameter values, forest_cv = function(n_folds,predictors, outcomes, forest_vars){ loss_per_fold = c() data_left = nrow(predictors) per_fold = data_left/n_folds empirical_losses = c() shuffle = sample(nrow(predictors)) pred = predictors[shuffle,] out = outcomes[shuffle] fold_train_in = data.frame() fold_train_out = data.frame() fold_test_in = data.frame() fold_test_out = data.frame() for(i in 1:n_folds){ if(data_left &lt; per_fold){ fold_train_in = pred[-(((i-1)*ceiling(per_fold)):nrow(pred)),] fold_train_out = out[-(((i-1)*ceiling(per_fold)):nrow(pred))] fold_test_in = pred[((i-1)*ceiling(per_fold)):nrow(pred),] fold_test_out = out[((i-1)*ceiling(per_fold)):nrow(pred)] }else{ fold_train_in = pred[-(((i-1)*ceiling(per_fold)):(i*ceiling(per_fold))),] fold_train_out = out[-(((i-1)*ceiling(per_fold)):(i*ceiling(per_fold)))] fold_test_in = pred[((i-1)*ceiling(per_fold)):(i*ceiling(per_fold)),] fold_test_out = out[((i-1)*ceiling(per_fold)):(i*ceiling(per_fold))] data_left = data_left - ceiling(per_fold) } forest = do.call(randomForest, c(list(x = fold_train_in, y = fold_train_out), as.list(forest_vars))) loss = data.frame(preds = predict(forest, fold_test_in), vals = fold_test_out) loss = loss %&gt;% transmute(loss_vals = case_when(preds == as.numeric(vals) ~ 0, TRUE ~ 1)) %&gt;% dplyr::pull(loss_vals) loss_per_fold = c(loss_per_fold, mean(loss)) } return(loss_per_fold) } The forest_cv function accepts a list or vector of named parameters to randomForest as input. To generate these, I create lists of each acceptable param value and just use what is effectively a cross join to create all possible combos (filter can be used afterwards if you have conditional dependencies between variables, even though I don’t). grid_param = list(ntree = seq(from = 1, to = 50, by = 2), mtry = seq(from = 1, to = 12, by = 1), nodesize = seq(from = 1, to = 10, by = 1), sampsize = seq(from = 80, to = 180, by = 5), replace = c(TRUE, FALSE)) %&gt;% expand.grid() Plugging these parameter values into the forest_cv function gives me a validation set loss for each set of parameters. Predictably, this takes a long while to run. Notice that, because training trees is stochastic, loss values can vary across each run. The large number of parameters means that it’s actually relatively difficult to search for a single configuration of parameters that is superior to any other. In fact, there were 9 of the parameter configurations which achieve zero error even on the validation set. filter(grid_loss, grid_loss == 0) %&gt;% select(!c(test_list, X)) ## grid_loss ntree mtry nodesize sampsize replace ## 1 0 43 1 8 95 TRUE ## 2 0 9 5 4 70 FALSE ## 3 0 17 1 1 95 FALSE ## 4 0 25 2 1 105 FALSE ## 5 0 9 2 4 110 FALSE ## 6 0 13 1 7 115 FALSE ## 7 0 25 2 6 120 FALSE ## 8 0 25 3 10 120 FALSE Obviously, these values are suggestive in that certain parameters seem to bunch around certain values. Ntree looks like it might be optimally 25, mtry 3, samplesize is mostly in the higher part of the possible range, replacement increases the variance too much, etc. However, any of those conclusions require a degree more of interpretation than would perhaps be desirable for a process which is supposed to be more automatic than other hyperparameter selection methods. The presence of a stochastic fitting method and a lot of grid search points - 66k to be exact - means there’s a very real threat that the grid search overfits to the validation set. We’ll return to the general question of test error later on, but one way to evaluate this is to look at broader plots of error across the range different variables like the one below: scatter_data = filter(grid_loss, mtry == 3, nodesize == 10, replace == FALSE)[,c(&quot;ntree&quot;, &quot;sampsize&quot;, &quot;grid_loss&quot;)] sc = scatterplot3d(scatter_data, pch = 16, color = &quot;steelblue&quot;, type = &quot;h&quot;, main = &quot;Validation Loss on 2 Parameters&quot;, xlab = &quot;# Trees&quot;, ylab = &quot;Bagged Sample Num&quot;, zlab = &quot;Loss&quot;) my.lm &lt;- lm(scatter_data$grid_loss ~ scatter_data$ntree + scatter_data$sampsize) sc$plane3d(my.lm) Looking at the points visually (and at the regression plane) seem to confirm the intuition the greater number of trees and larger samples are good. However, it is easy to look at any region in the plot and find points which have low error. Of course, this doesn’t mean grid search has hopelessly overfit per se. Just that one can find evidence that many configurations of hyperparameters are efficient if one wants to and that a more judicious procedure is necessary for selecting the final grid point. 1.4.2 Gaussian Process Search I use the mlrMBO package for Gaussian Process search. This package wraps the ParamHelper, smoof, and DiceKriging packages which implement most of the necessary logic. The MBO function accepts 3 types of objects: a learner which determines the next step to take in the parameter space, a design matrix which gives the priors over the space, and a control object which determines termination and local search depth. In my case, I sample 20 points uniformly from the parameter space, and allow a 10 iteration focus search to identify a locally optimal point. I allow the process to run for four iterations. forest_obj = smoof::makeSingleObjectiveFunction(name = &quot;Gaussian Process Forest&quot;, fn = forest_wrapper, has.simple.signature = FALSE, par.set = makeParamSet( makeIntegerParam(id = &quot;ntree&quot;, lower = 1, upper = 50), makeIntegerParam(id = &quot;mtry&quot;, lower = 1, upper = 12), makeIntegerParam(id = &quot;nodesize&quot;, lower = 1, upper = 10), makeIntegerParam(id = &quot;sampsize&quot;, lower = 70, upper = 120)) ) ctrl = makeMBOControl(propose.points = 1) %&gt;% setMBOControlTermination(iters = 5) %&gt;% setMBOControlInfill(crit = crit.ei, opt.focussearch.points = 10) des = generateDesign(n = 20, par.set = getParamSet(forest_obj), fun = lhs::randomLHS) lrn = makeLearner(cl = &quot;regr.km&quot;, predict.type = &quot;se&quot;, covtype = &quot;gauss&quot;) mbo(fun = forest_obj, design = des, control = ctrl, learner = lrn, show.info = TRUE) This code block is going to output an absolutely huge amount of data about the optimization path which is frankly rather hard to read and completely irrelevant. Instead, I’m going to copy over literally the only line of the output which actually matters: ## Recommended parameters: ## ntree=31; mtry=3; nodesize=6; sampsize=99 ## Objective: y = 0.018 This result seems very reasonable based on our previous investigation of what optimal parameters might be. Perhaps the number of trees is too large or sample size is too small, but the values are close. The validation loss (“Objective” in the output) is higher than comparable objectives in the previous section, but this could be expected given the previous discussion on overfitting. Notice that there isn’t really that much interpretation for me to be doing here: I run the algorithm and the fact that I’m doing many less function evaluations means that there’s no huge space of results I need to consider. I just pick the one with the lowest loss. 1.5 Discussion The most notable thing here is that I have gotten a similar result to grid search with the gaussian process only by doing 4 iterations that evaluate a maximum of 10 points each. This is a miniscule fraction of the 66k evaluations that I did in the grid search. I know I said I wouldn’t talk about time, but it is worth mentioning that the grid search took 45ish minutes while the gaussian process took easily less than a minute. The easy conclusion is that gaussian processes are a lot more efficient at finding approximate solutions that are near the minima but might not be the best compared to an exhaustive search. However, this analysis neglects the overfitting concerns that imply that the lower bounds on the validation error for exhaustive type searchs are misrepresentative of the true test error. I’ve held out a test set of 20 observations; let’s take a look at the test errors there compared to the test errors on the recommended model from gaussian process regression. grid_loss %&gt;% filter(grid_loss == 0) %&gt;% rename(`Test_Loss` = test_list) %&gt;% select(!c(grid_loss, replace, X)) ## ntree mtry nodesize sampsize Test_Loss ## 1 43 1 8 95 0.00 ## 2 9 5 4 70 0.05 ## 3 17 1 1 95 0.05 ## 4 25 2 1 105 0.00 ## 5 9 2 4 110 0.00 ## 6 13 1 7 115 0.00 ## 7 25 2 6 120 0.00 ## 8 25 3 10 120 0.00 Probably unsurprisingly, these models do a good job of describing the data and most have zero test loss also. However, a couple of them do manage to misclassify an observation or two. When we run the model that the gaussian process selects we get a test error of zero which is exactly in line with what we get from the best models in grid search. There are two points to be made here. First of all, it’s hard to say exactly how trustworthy this result is because of the sample size. 20 observations isn’t very much and the power of this test isn’t very high so it could be that the gaussian search model is overrated. That said, the structure of the gaussian process which encourages fewer evaluations could also be viewed as regularizing which would have the tendency to increase its performance on test sets. Obviously it has less information about the performance on the training set than the grid search but that doesn’t necessarily imply lesser performance on the observations we care about. All this gives a pretty good account of gaussian processes as a method; I’d like to just point out a couple more things that make them desirable. For one, they adapt better to increasing dimensionality than grid search does. The amount of evaluations for grid search is exponential in the dimension of the data so it’s not like the competition is hard here. It’s very hard to put bounds on the performance of gaussian processes because it mostly depends on how well the funtion concords with the kernel but empirically convergence is relatively fast and the local search algorithms they use have polynomial bounds for sufficiently regular functions (like most loss functions). They also are able to get a better local estimate of the optimal hyperparamter arrangement than grid search. Grid search inevitably allows gaps between points in order to make the number of evaluations tractable. These gaps can become rather large (especially when the dimension is high) and naive grid search methods have no way to interpolate between them. Gaussian processes aren’t restricted this way and can narrow in on a very specific local estimate at any value allowing them to get a better outcome in the limit than grid search. I’m not going to pretend like there are all positives. Gaussian process search is rather finicky and there aren’t many packages that do it in R. The ones that do tend to all have their own specific type of objects that they like and accept and don’t play nice with others. It can get complicated and is almost definitely overkill for small projects. However, when hyperparameter searches are important and function evaluations are expensive, this is a great option that minimizes the search time and is highly tunable for your specific purpose. "],["the-effect-of-increased-campaign-spending-on-election-results.html", "2 The Effect of Increased Campaign Spending on Election Results 2.1 Background 2.2 Motivating Trends 2.3 Initial Regression Analysis 2.4 IV Regression 2.5 Discussion &amp; Conclusion", " 2 The Effect of Increased Campaign Spending on Election Results 2.1 Background Much of the discourse surrounding money in politics and campaign finance in the US make it seem like money is a large determining factor in the outcomes of elections. Representatives certainly believe this to be the case (otherwise they wouldn’t spend a large portion of their days making what, by all accounts, are mind-numbing calls to donors) and neither do operatives who make strategic choices about where to spend money (I myself have heard from one’s mouth that they are positive they can influence elections by spending more). However, the academic consensus on this question is much more open (Levitt 1994, Ansolabehere et al. 2003).One thing is relatively clear: candidates who have money do win elections at an astonishingly high rate. This has been a consistent finding since spending has been measured and yields an easy explanation: campaign spending plays a decisive role either in driving turnout or in influencing public opinion. However, this “obvious” explanation conceals an equally obvious methodological problem. Candidates who raised more money and won their election might have won their election because they indeed were able to spend more or, alternatively, they might have been able to raise more because they were a better candidate and some amount of money on the margin had little to no effect (note that, while there are plenty of low visibility candidates from small parties who raised little money and got few votes, we can only really measure marginal effects of spending on votes for high quality candidates). This causal problem creates a substantial public policy issue because it becomes difficult to ascertain to what extent money actually influences election results and thus complicates debates on if policy intervention in federal elections are appropriate. I look at house elections from 2010 to 2022 in this analysis mostly because house election are relatively homogeneous and have substantially more elections than the Senate or Presidency. State reporting of election financing is spotty and regulations involving election financing vary wildly which makes them a less appealing focus of study compared to Federal elections. 2.2 Motivating Trends The most obvious conceptual model of campaign spending is as a simultaneous-equation-model (SEM). Elected officials don’t like to fundraise and have a lot of things to do in the meantime. Challengers’ decision to fundraise trades off with other activities that can support their candidacy like engagement with the community, interviews, etc. So, incumbents observe the potential threat that the challenger poses and decides to set a fundraising based on that while candidates decide on fundraising levels based on their perceived returns and projections of incumbent spending (from a game theory perspective, this is actually an iterated game, but the SEM approximation is good enough). To identify this model, we would need exogenous variation in spending for both types of candidates. However, while conceptually pleasing, there is reasonable heuristic evidence to suggest this conceptual model isn’t entirely accurate. To illustrate this, we pivot our focus from proportions of winners to log raw money spent (total spending has a thick tail) which reveals some interesting trends: There are clearly different effects of money in different parts of this graph so I’m going to break it down section by section. Firstly, most election losers spent less than average and the amount of their spending is clearly correlated with their vote share. The most money was spent by those candidates who had vote shares around 50%. These elections are the most competitive and where we’d expect the most spending to take place because an additional $50k or so has the highest chance to change the election outcome. For those candidates who scored substantially above 50% of the vote, there appears to be no relation between vote share and amount spent. Notably, candidates in this zone generally outraised candidates who achieved less than 50% of the vote and have a much more consistent level of funding across candidates. Most of this disparity can be explained by the advantage of incumbency. Another persistent fact of American elections is that incumbents win elections: they win elections a lot. In fact much of the variation in the previous graph can be explained by whether or not incumbents won reelection that year. Redoing the previous plot with points colored by incumbency shows that almost all of the points in the upper part of the graph are incumbents. There are many possible reasons why incumbents have such a persistent advantage in fundraising. For one, incumbents are probably higher quality candidates on average than challengers given that they have greater experience and that they have won elections before which possibly drives higher individual donations. Incumbents also have higher name recognition due to their political service which they can leverage to fundraise. The reality of incumbency advantage can also drive donations to incumbents. If a company or wealthy individual wants to influence policy or buy access to politicians but has a limited budget to do so, they are most likely to funnel their donations to the candidates which are most likely to win: which are the incumbents. This reality means that money probably has very different effectiveness depending on how competitive a specific race is. For example, candidates which raise little money and get little of the vote are probably in a situation where more money tangibly causes more votes because it can buy them the visibility which they so desperately need: however, they are unable to drive donations to the level which would buy them widespread visibility and are probably much lower quality candidates than incumbents and would face severe diminishing returns on increased spending. In contrast, incumbents seem to spend close to the maximum observed amount regardless of whether or not they are anticipating a strong challenger in the general election. Much of this can be explained by the fact that campaign donations cannot be saved so candidates are obligated to spend all the money they get. If incumbent fundraising can be thought of as either an effort to retain connections with key donors for potentially threatening races in the future or an effort by donors to buy favor with the likely winner, it makes sense that incumbent fundraising is both high and relatively inflexible in the face of large variations in the strength of competition. This analysis provides a different way to interpret the more obvious way of contextualizing a candidates spending: as a percentage of the total spending in that race (spending is adversarial so low amounts of spending isn’t necessarily indicative of an advertising advantage if the other candidate spends little as well). The relationship in the graph is linear and looks more clearly like a relationship one would expect if money caused candidates to win elections. However, one also observes a similar incumbent-challenger distribution over the spending measure to the previous graph. The relative invariance of incumbent spending suggests that this relationship is mostly a function of the relationship between challenger vote share and fundraising: this is hard to interpret as a causal effect of spending on vote outcomes. These facts motivate a restriction of the data to close elections (defined as elections where both candidates have a vote percentage within some bandwidth). There is a range of vote percentages around 50% within which incumbents and challengers seems to equally mixed. This is also the same range within which the maximum average amount of spending was observed over all different vote percentages. This analysis suggests that it is these elections where there are competent challengers who can fundraise and in which incumbents choose to juice their fundraising apparatus above what might be normally expected. Thus, we expect that marginal increases in spending are because candidates believe that spending will tangibly affect the vote which is the effect we want to identify. 2.3 Initial Regression Analysis The FEC reports a wide range of numbers about the fundraising for each candidates campaigns. Each election cycle (every 2 years) those candidates who ran for office are required to issue disclosure reports (in tandem with more granular reports on the way) regarding their fundraising, spending, cash on hand, etc which are in turn published. Variables are drawn from this dataset. A naive model would regress cash raised against vote shares with year and district fixed effects along with a control matrix to predict vote share. I’m going to run this regression and then explain why it is bad in lieu of a more robust regression based approach. The control variables are worth talking about because there are a lot of confounders in this dataset and multiple ways that we can at least approximate the confounders using the data available. This way we can obtain a regression that is at least mildly robust against many biases one could expect in a specification such as this. To start, one possibility is that higher candidate quality acts to dissuade primary competition. Since funding capacity for a candidate is probably mostly fixed, not needing to spend money in the primary could create an observed monetary and vote advantage for higher quality candidates through primary competition. One possible way to control for this is to add vote percentage during the primary as a control to the regression. However, given that primary competition should moderate the effect of increased fundraising, it makes more sense to interact the two terms in the regression. Unsurprisingly, general election and primary election shares are correlated, but not that strongly and whether that relationship persists after other variables are differenced out is a regression question. The general political beliefs of local populations also tends to remain relatively constant between election cycles. If there are systematic differences between election performance and fundraising between the parties, this might cause an erroneous positive coefficient. So, we use the performance in the previous election of the member of the same party to control for any party bias inherent to the district. Some parties also just perform better in certain years than others due to political circumstances that can’t be explicitly modeled. To try to estimate these, I include the aforementioned year-party fixed effects in the model. As mentioned earlier, perception of competition is an important determinant of candidate behavior. One way to possibly measure the candidates perception is the difference between what a candidates raises and what they spend. Those who leave a lot of money unspent are unlikely to feel they are facing a tough competitor. The same goes for donations from the party who are only likely to donate to elections they think they have a chance of swaying. Interacting these measures of competition with disbursements allows for a variable effect of disbursements depending on candidates perception of competition. Variables like party, incumbency status, and other relevant controls are also present as controls. The basic specification is as follows: \\[\\begin{equation} Vote\\%_{y,c} = Spending_{y,c} + FE_{d,y-1} + X_{y,c} + I_{y,c} + \\epsilon_{y,c} \\end{equation}\\] where I is the aforementioned interaction terms, y,c, and d are year, candidate, and district respectively, and epsilon is assumed to be i.i.d. gaussian. Regression results for the naive regression are reported below: Clearly there is a (very) significant positive coefficient between money a candidate has received and election share. However, as outlined above, this is in no way indicative of election success. Also worth noting is that this regression is generally terrible. There should be some regularization to enable easier feature selection and some variety of bootstrapping to avoid overfitting. But that would a bit overkill at this point so I’m going to do that in the next section. 2.4 IV Regression There are a few ways to identify a causal effect of money on election success. The cleanest way would perhaps be to look at state elections where one state has passed a fundraising law that differentially affects some candidates and not other (think raising some cap on union donations to PACs or a policy like that) which would allow us to do DnD between those two states. Unfortunately, state records tend to be spotty and I’m not aware of any such clean variation where data is also available. So, I resort to a next-best option: an IV regression. In this case, the instrument that I use is personal wealth. Candidates which are wealthier have more money and are able to donate to their own campaigns to increase their war chests in a way which is not indicative of public support. In fact, there seems to be little evidence that wealthy candidates do disproportionately well (besides the fact that basically all candidates have to be at least somewhat well off to take time off work to campaign in the first place). Instrument validity is relatively simple. The relevancy criteria is obviously met because more donations by the candidate directly lead to more spending capability by the campaign (this directness allows us to dodge the mental gymnastics that often characterize relevancy arguments). The main worry that we have to contend with in the exclusion requirement is that candidates who donate to themselves more also receive more donations from the public. A story might sound something like: candidates who can donate money to themselves use it as seed money to run advertisements which encourage more donations and create a virtuous cycle which isn’t available to self-funded candidates. Fortunately, there appears to be no relationship between different types of contributions: Running a regression between these two quantities also yields a negligible coefficient. The actual regression specification is similar to the previous one except TSLS is used where candidate donations are instrumented using candidate self-donations and the control vector. This regression is difficult to interpret as there are a ton of variables from the interaction terms. I tried a couple methods to keep model complexity low: namely LASSO and forwards-stepwise regression with bootstrapped error and an AIC selection criteria: however, none of these returned models with non-zero coefficients for the instrument, so I choose to report the whole model. There are a couple reasons this could be happening: 1) is that there is simply no relationship after the variable has been instrumented, or 2) the instrument simple isn’t powerful enough. Obviously we can’t rule out number 2, but the wide variety of candidates who donate to themselves comparable amounts as they receive in donations suggests that this effect isn’t important. Anyways, here’s the regression table for the instrumented regression: As can be seen, the instrumented coefficient is no longer significant providing evidence that the observed effect of money on vote share is most likely a result of OVB wrt candidate quality. 2.5 Discussion &amp; Conclusion The effect of money on politics is a topic which is highly controversial in our current democracy. The complaints often assume that money plays a determinative role in who gets elected. However, this is a claim that is particularly hard to marshall evidence for and this analysis would seem to suggest that, on the margin, more dollars, in and of themselves, don’t meaningfully contribute to the vote. Of course, this only measures marginal increases in fundraising in the house in certain years in competitive races and therefore could be subject to a variety of biases. The most concerning is that much money is funneled through a variety of PACs where spending can’t be measured due to a variety of reporting requirements and the level of this spending could be correlated with the instrument. This concerns are perhaps less severe than they would be for Senate or Presidential races as they get less funding and less attention than said races. However, it is still a real concerning source of bias. Nonetheless, the directness of the instrument and the fact that many donors almost double their individual contribution levels through donations means that the regression is strongly suggestive of a more complicated story about the effect of money. "],["neural-network-methods-for-electricity-demand-forecasting.html", "3 Neural Network Methods for Electricity Demand Forecasting 3.1 Introduction 3.2 Common Neural Network Designs for Forecasting 3.3 Problem Statement and Data 3.4 LSTM Results 3.5 CNN Results 3.6 Analysis", " 3 Neural Network Methods for Electricity Demand Forecasting 3.1 Introduction Electricity demand estimation is a classic problem in machine learning with many papers addressing potential approaches. The classical approach in the literature for time series estimation are CART methods where suitably deep trees can model the discontinuities induced by sudden changes in weather. However, this is an open problem that permits a variety of solutions such as classical time series models like exponential smoothing with a variety of seasonal adjustments or Monte Carlo simulations of weather models as the basis for ensemble learning. Not wanting to commit myself to simulating weather dynamics, I’m going to investigate time series forecasting of the local power demand in New England using several common neural network models for forecasting and compare their performance. 3.2 Common Neural Network Designs for Forecasting While there are an incredibly large class of models that can learn time dependencies (and innumerable additional variations on each broader class), there are only two main neural network architectures which are commonly recommended for general purpose time-series forecasting. The first are varieties of recurrent Neural Networks (RNNs) which learn how to create long term representations in memory of previous events. The most popular variants thereof are GRU networks and it’s more complicated, but more popular, cousin, the rather confusingly named long short-term memory network or LSTM. LSTMs blocks consist of a single layer which is unwound to learn a series of inputs in the manner of all similar RNNs. However, in addition to learning a weight matrix that maps into its latent space, LSTMs also learn a series of mappings into and out of a memory unit. This memory unit can be of an arbitrary dimensionality and the LSTM learns dependencies upon the memory unit which allow the content of the input to alter the memory unit (ie remember a current feature or forget a feature it has remembered in the past) or allows the memory unit’s content to alter current predictions. The LSTM learns these mappings in the usual RNN fashion of backpropogation through time though the number of number of parameters to learn is much higher per latent dimension due to the existence of the memory cell. These extra connections allows LSTMs to learn more complicated mappings back in time; functionally, LSTMs can be thought of as an extension on ResNet type architectures where LSTMs can learn more complicated types of skip connections. In fact, before transformers came into vogue for learning text embeddings, LSTMs were a popular option because of their ability to learn long term dependencies between different types of words in a sentence. It is this same property to learn longer term dependencies that we wish to exploit for short term power forecasting. The second primary method for learning time dependencies are convolution neural networks (CNNs). CNNs originated from adaptations of filtering techniques in image processing to neural networks; as a result, CNNs are primarily popular for their applications to image classification tasks. The traditional CNN design alternates filter layers where an arbitrary number of discrete filters from classical image processing (think sharpening or blurring filters) are convolved with different color layers of an image to produce feature mappings. These features are then fed into pooling layers which are essentially dimensionality reduction techniques that attempt to preserve spatial relationships between features. By chaining these layers together, one can build feature representations that are roughly translation invariant which is good for image classification where translation invariance is an important property. These same techniques can also by applied to applied to time series analysis by reducing the input dimension by one. Whereas in image classification 2d filters are applied across different color channels, in time series 1d filter are applied across different input features while pooling and fully connected layers are identical. While translation invariance and pooling layers are less important here, the local nature of filter layers allow the network to learn to identify types of events that have heterogenous future effects on the series and then propogate those forwards in the fully connected layers. 3.3 Problem Statement and Data Short term electricity demand forecasting is an important task for managerial authorities who control the transmission of electricity and must respond to changing consumer demand and generation authorities who must make decision about whether it is profitable to turn plants on or off at certain times. Having robust models for all these parties allows operators to make more informed decisions about aligning supply with demand and thus creates more efficient market outcomes while avoiding problems like rolling blackouts or brownouts due to underprovision. We attempt to forecast per state demand in New England using previous values of demand in the relevant state and in neighboring states. Given that demand is highly dependent on temperature and thus time of year, the actual timestamps on the time series can also be used as features. In this case, we transform the month and hour components using sin-cosine transforms and use those as feature inputs. Our data is from the Energy Information Administration which, unsurprisingly, publishes energy demand information. Specifically, the EIA publishes per hour demand information broken down by specific state, grid authority, and region. We use the EIA API to pull a three year stretch of demand information over a group of New England states which we then plug into our neural network designs. I train on short term prediction accuracy, ie how good is the network at predicting the next hour of demand. This is a relatively approachable task that demands the network learn both trends over time, level changes over seasons, and short terms changes across days (ie over 24 or so samples). 3.4 LSTM Results The main parameter to tune in LSTMs is the number of latent features. This allows the network to learn more complex feature representations across time. However, the number of parameters that arise from an increase in feature dimension is large meaning such an increase must be used sparingly if training time is an issue. Take this Keras printout from a relatively simple network of an LSTM layer followed by a fully connected layer which predicts one output: ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## lstm (LSTM) (1, 20) 2480 ## dense_1 (Dense) (1, 20) 420 ## dense (Dense) (1, 1) 21 ## ================================================================================ ## Total params: 2,921 ## Trainable params: 2,921 ## Non-trainable params: 0 ## ________________________________________________________________________________ The number of parameters are 5x greater for the LSTM layer despite having the exact same input and output dimension as the fully connected layer; this fact motivates designs that include fewer LSTM layers the number/dimensions of LSTM layers. In this instance, I will only be using 1 LSTM layer. Hierarchical LSTMs which use chained LSTMs to generate results are popular in some applications but are not strictly necessary to create sufficient model complexity due to the fact that LSTMs are recurrent and effectively generate depth by unwinding through time. The base model I use is an LSTM layer followed by a batch normalization and two fully connected layers with weight dropout as regularization. I first compare the efficacy of different LSTMs with for different hidden dimensions. Theoretically, more dimensions may allow the LSTM to model more complicated time trends: however, greater dimension also makes it substantially harder to train and prone to overfitting the test data more rapidly. We test the same data with LSTMs of dimension 8, 32, and 128, with mixed results. As can be seen below, the two smaller models perform comparably on the test data while the larger model starts to overfit around 25 epochs and is substantially more volatile. The most probable reason for this is a learning rate that is too large (.01 in this case). One of the nice results that comes out of the literature equating SGD updates to stochastic differential equation flows is that lower learning rates lead to greater approximations to the ideal SDE and thus lesser instability than what we see while more parameters pushes in the opposite direction (Jastrzębski et al 2018 is a great paper on this). I would say that it’s highly probable the 128 unit model would perform at a lower learning rate, but I don’t have the heart (or the time) to put my poor laptop through a grid search of said model, so I’m going to be looking at the 32 dimension model. Given that we only want to compare performance to CNNs and I’m definitely not going to be doing extensive hyperparameter tuning there (the Keras function has like 20 arguments), I’d say this permissible. That said, learning rate is probably going to be the other main parameter beside model size that determines how well the model fits, so we’re going to check a few values for the 32 unit model to see how well the model generalizes. Given that the learning rate was probably too high for the 32 unit model judging by the slight instability in the training data, we try two smaller values. It is clear that a learning rate of .001 wins out in this case and it seems that we hit it more or less on the head as both other learning rates exceed the test loss. Notably, for the blue .001 loss, the MSE has a valley at around 25 epochs and spikes upwards. We can say that playing around with higher epoch counts, early stopping is highly beneficial in this context as it has the capacity to overfit rather strongly otherwise. Note that the loss is MSE rather than MAE and the data is standardized which means that average error is around .3 standard deviations for the optimal model, which, all in all, isn’t terrible and serves as a reasonable benchmark with which to compare CNN architectures. 3.5 CNN Results Remember when I said that I wasn’t going to do grid search on CNN parameters? I meant that. I could spend all my day (and computing power) sitting around playing with stride, activation functions, pooling layer width, kernel size, padding policy, etc. I’m not going to do that because it sounds boring. Also, that not quite the point here when I only want to get a rough comparison between LSTMs and CNNs. The thing about CNNs is that they don’t have complicated state transition maps which means that they don’t require comparable parameters to a LSTM for the same depth. The first thing we test for CNNs is also model complexity and to make things fair we’ve decided to create architectures with comparable parameters counts. Note that counting parameters isn’t a perfect solution here - models take different amounts of time to train and offer different amounts of flexibility per parameter. If increased parameter count concerns us because of increased training time, trying to measure said training time and equate it between models creates a very difficult situation where results are highly heterogenous between computer systems with different memory architectures despite identical algorithms (on top of the obvious semantic problems). If it concerns us because we want to equate model expressivity, the explicit bounds in the literature are both loose and very narrow while trying to measure effective degrees of freedom through some sort of matrix method (think smoothing splines) requires us to train models and identify global minima: which we can’t do. So, we just take the naive approach and assume that equality of parameters roughly implies equality of other quantities of interest. The structure of our CNN model is relatively simple. Two convolution layers with kernels of size 19 and 11 are interspersed with 2 max pooling layers with kernels of size 3, which is followed by a fully connected layer that is linked to the output. The number of filters for each of the convolution layers is varied in order to achieve rough equality in the number of parameters between the comparable LSTM models and the CNNs. After specifying the model, we run into a big issue that isn’t present in LSTMs: the decision about how many lags to include when training the CNN. Unlike LSTMs where past dependencies are only limited by how long your data goes back, CNNs require that all lags where one wishes to have dependencies be fed in at time of prediction. In some ways this is beneficial as it allows greater parralellism and thus greater batch sizes (whether this is desirable is debatable) that speed up training. The flip side is that the memory costs are much higher for CNNs, especially at higher batch sizes as the input requirements mean data duplication is almost always necessary. We had the frustrating experience of getting a memory overflow error a couple of time during training what is a relatively small dataset for this project (admittedly without a GPU or much RAM, but still). This consideration is probably going to be one that will dominate your thinking when choosing a specific model rather than efficiency concerns and the memory restrictions in our case means we can only model dependencies for a lag of up to 120 samples. A 120 sample restriction means it will be harder to model long term dependencies: let’s see if that show up in the loss. The performance is not very good; especially for the larger model which finds itself a nice local minima in the training data (despite all my prodding to make it not do that) and overfits almost immediately on the test data. The medium size model performs a bit less well on the training data but it seems like it might generalize slightly better even if it’s not overparametrized so I’ll use that to test different learning rates on. ## Warning: Removed 1 row containing missing values (`geom_line()`). ## Removed 1 row containing missing values (`geom_line()`). The lower learning rates are comparable to each other while the higher one clearly struggles and bounces around in the training data. For the test data, they all suck. The optimal LSTM model had a test error (with early stopping) of around 1/2. The best CNN is 50% greater than that. Perhaps this is a problem that could be corrected with enough training but there’s no a priori reason to believe an LSTM wouldn’t also improve a commensurate amount. 3.6 Analysis This analysis seems to indicate that LSTMs outperform CNNs on this specific dataset. This fact is not terribly surprising. We’ve already outlined how data limitations on CNNs make them less useful when modeling longer term dependencies. In addition, the kernel-pooling structure of typical CNNs is made to create translation invariance in image classification tasks which is not what one necessarily wants in a time-series regression context. If there’s one good lesson here it’s probably that CNNs really aren’t as good as LSTMs for this specific sort of task but that one should really let one’s problem domain guide model choice. Technically, CNNs and LSTMs are both models that can function in a time-series context. However, when one needs to model longer term dependencies (like fluctuations in power demand over the week when data is this granular), CNNs have specific constraints that make them difficult to use from an efficiency and memory perspective. However, I suspect that if one only needed to model short term dependencies, CNNs would be more efficient as they more explicitly model connections. And of course, this is only one experiment on the two most common sorts of models that are recommended for these sorts of tasks. One could also use an explicit time-series model (ARMA or the like), a random forest, smoothing splines, or even the combined CNN-LSTM models which are often held up as more efficient than either one. All that said, there are reasons why people don’t readily apply certain types of models to data like this and if you choose to use a hyper-flexible model for time series, it should be because there are a multitude of factors that weigh in favor of its use. "],["the-effect-of-seed-market-competition-on-farmers.html", "4 The Effect of Seed Market Competition on Farmers", " 4 The Effect of Seed Market Competition on Farmers This is my bachelor’s thesis in economics. I don’t have the original Latex or figures right now to recompile the thing here, so I’ve just embedded the pdf for you viewing edification. "],["chicago-taxi-demand-estimation.html", "5 Chicago Taxi Demand Estimation 5.1 Introduction 5.2 Full Data Procedures", " 5 Chicago Taxi Demand Estimation 5.1 Introduction Demand estimation is an important topic for businesses of any kind: accurate demand forecasts can allow them to operate on higher margins while poor forecasts risk either disappointing clients due to a lack of supply or saddling the business with excessive or labor without a buyer for the product. While the input for the taxi market is just labor, the analysis is no different. Fortunately, the City of Chicago has a dataset where they release data on the time rounded to the nearest quarter hour, location, and duration of literally every taxi ride in the city (or at least that’s what they say). This dataset allows us to do fairly robust demand estimation due to its granularity. The most important decision for a taxi decision for a taxi firm is a) when to hire more drivers, and b) when to schedule more drivers. To this end, I will be doing both seasonal and daily trend predictions. Unfortunately, observed quantity is endogenous. Attempts to supply more taxis within the data period will increase observed quantity (which I might erroneously interpret as increased demand): fortunatley, the data allows reparametrizations that decrease these concerns which I will explore at the end. 5.1.1 Data The dataset provided by the City of Chicago is quite robust in that it appears to be quite clean already. The problem is that each row is an individual ride and the timespan of the dataset is 2002 to present. Chicago is a big city with a lot of people and a substantial number of taxis. The resulting dataset has over 200 million observations. My computer is puny and even SQL queries to a local server are very slow. So, I’ll be first analyzing 2022 data as an outline of our methods and approaches and then extend this analysis and restimate models with multiple years later. The dataset also allows multiple ways to estimate demand. Ride data comes with length, time, and fare data which might allow us to create a more accurate picture of demand. For the first part, we will use count data, but then diversify to other types of demand measures later. 5.1.2 Trends There is multiple levels on which one can examine demand in this dataset. The most trivial would be to examine raw number of rides within a timespan. There are already some interesting trends in this data at first glance. For one, there is a solid trend upwards through most of the study period until around November at which point the demand seems to dip back down. Chicago gets very cold in winters and it makes sense that demand would be negatively affected during that period. However, demand does not seem to achieve the lows that it did at the start of the previous year which would suggest long term growth in the demand for taxis. From a technical perspective, the demand is definitely not stationary and is looking like it might not even be first-difference stationary. On a slightly different topic, the variance of the measurements increase as the year goes on which has two separate implications: OLS isn’t going to be enough here. If we wanted to use regression we would need something like a quasi-likelihood model that controls for heteroskedasicity in order to get meaningful confidence intervals. Secondly, if we want to use some sort of exponential smoothing model, the increased variance suggests that the right approach is to use one with multiplicative trends as opposed to additive ones (more on this later). The next important question is what kind of seasonality is there in this data. We’ll address year seasonality later, but right now we can visualize daily and weekly seasonality already present in the data. The previous graph shows a 100 tap moving average filter that is fitted to demand measurements at the lowest granualarity: counts of rides numbers in 15 minute bins supplied in the dataset. The distinct peaks that we can see correspond to weeklong periods. Plotting average number of rides by weekday gives the following graph: If you look on the y axis, the differences between low volume days and high volume days is only 10% or so, but this is still enough to create a noticeable seasonal trend that shows up in the weighted average. There are also clear seasonal trends across days in the day that look roughly similar to a sine wave. We can group data by time of day (without controlling for anything else) and get the following visualization: This isn’t quite sinusoidal, but it’s close, and it’s about what we would expect if demand was a smooth function of time of day. Note that, once again linear regression isn’t an appropriate choice here. Even polynomial regression is undesirable as the Taylor series of a sine wave converges rather slowly on the boundary of a compact interval centered on the point of expansion compared to the center of the set which results in more parameters than necessary. Not to spoil the fun, but there are going to be seasonal yearly trends in this dataset too which complicate our ability to apply simple estimation procedures. 5.1.3 Possible Estimation Procedures Not to beat a dead horse here, but our data is not stationary. Applying a KPSS test with a null hypothesis that the data is trend stationary gives a significant p-value. One of the primary reasons for this seems to be the seasonality which we have a variety of ways to deal with. ## ## KPSS Test for Trend Stationarity ## ## data: as.numeric(daily_rides_2022$num_rides) ## KPSS Trend = 0.29748, Truncation lag parameter = 16, p-value = 0.01 The basic approach to seasonal time-series modeling is to use some sort of moderately flexible exponential-smoothing/FIR model that has the ability to remove a seasonality or trend component specified by the researcher. SARIMA, Holt-Winters, and periodogram based approaches are all examples. The problem with all the previous approaches is that they only allow the specification of one seasonal component. Obviously, one could estimate seasonal components separately through use of some sort of restricted AR model and then use a standard estimation technique on the residuals. However, this approach is undesirable as having models with multiple levels are hard to debug and allow model error to more easily be propagated through residuals as opposed to estimating all the parameters simultaneously. This means a more flexible model is needed in order to fit. One upside of our data is that we can be pretty sure that season periods are fixed. This means that flexible season length methods like STL aren’t strictly necessary, but it actually ends up to work pretty well here so that’s what we’re going to use. The main challenge in working with this data is the nonconstant variance structure which limits the effectiveness of highly parametric methods and pushes us towards more flexible local methods. There are a few possible ways to deal with this issue. The easiest is to transform the data to create a more desirable variance structure: the most common transformation is obviously the logarithm where the compressive nature of the log pushes variances closer together (this also has the benefit of turning multiplicative relationships into additive ones). Taking the log actually works pretty well here and generates data that looks like it has normal errors but doesn’t necessarily mean that Holt-Winters performs well. Another possible approach is to model the variance explicitly: the easiest method would be fitting some sort of quasi-likelihood model to the time series. However, we find this approach generally suspicious unless one has a prior that the variance conforms to a set structure. There also isn’t much literature on this approach and I don’t want to code a package that implements something like that…so there you go. Luckily, the dual approach to this is to use a method that is flexible enough to model the variance itself which is well studied and has a few good algorithms for time-series. We’ve already spoiled that we’re choosing the third option and using STL to perform the trend/seasonality decomposition. So, let’s get to it. 5.1.4 STL Decomposition One challenge with forecasting in the smaller context we’re operating in right now is that we don’t know if the dip down at the end of the period is a yearly seasonal effect or just an exogenous shock. There’s not a ton to do about this right now, but just be aware there’s some grains of salt to be had until we move to the full analysis. As is easily apparent, the STL decomposition for the trend is pretty gritty, but that’s a benefit insomuch as it allows us to get a stable estimate of the seasonal component that changes relatively little over the course of the study period. Basically all modern time series procedures involve a decomposition of a series into trend-cycle, seasonal, and irregular elements and the estimation of those elements individually. STL gives me that decomposition that I can then play with in order to get a good approximation of the next few periods. The strategy is pretty simple. The last few samples of the seasonal decomposition give me an estimate of the trend-cycle at that point (this can be estimated using a constant or interpolating polynomials (I use interpolating loosely here), where I choose the latter). Seasonality can be estimated simply repeating the next few samples of what it would have been from the previous season and the irregularities can be estimated using a feedforward ARIMA design. A nice benefit of the ARIMA approach to estimating irregularities is that it preserves variance without transforming the data thus aiding in interprebility. Here I present a ten day forecast for demand. You can also theoretically do the same procedure for hourly demand with a less flexible LOESS selection and day seasonality on top of week seasonality. Note that the confidence intervals are only the sum of the natural intervals from the ARIMA model and the prediction intervals from the interpolating polynomial (note that the constant residual variance assumption for the prediction intervals is approximately satisfied since we’re only pulling from a rough period near the end of the series). Note that this specification doesn’t guard against model error and the projection would have done a very poor job of predicting the downturn if they had been taken a few weeks prior. In that sense, the confidence intervals are very optimistic (especially considering that we will see there is yearly seasonality that hasn’t been accounted for). However, they do provide an easy an decomposable heuristic for prediction which we will undertake in the next section with the whole data. 5.2 Full Data Procedures The full data contains all taxi rides from 2013 to 2022 in Chicago and allows us to decompose year trends from trend-cycle data. A few things are notable about the full data. For one, yearly seasonality is easy to observe in this context where demand peaks during the middle of the year and tapers off in the colder months (when it is perhaps understandable that people have less desire to be outside in the Chicago winter). Another important note is that the start of the coronavirus pandemic is easy to observe near the beginning of 2020 and has a clear effect on both the level and the variance of the data. Longer term trends in demand are also present in the data. Demand seems to rise between 2013 and 2015 while declining before the pandemic (perhaps due to greater competition from rideshare services or due to demographic trends in Chicago itself). It is also easy to observe a level effect on the variance that is most apparent in the summer months and post-Covid. Given the our trend-cycle estimates come from STL which is a fairly flexible method, we really don’t need to modify the previous procedure except to add seasonality (besides that fact that I’m going to be waiting around a lot longer for my computer to plod through all the data). So, let’s do that. Training is essentially identical and we get an STL decomposition, a trained ARIMA model, and a fitting procedure after all is said and done. Let’s see how the model forecasts the last week of the data compared to the actual data: As can be seen, the estimation method tracks relatively well with the actual data. Obviously, much of this is owed to the relatively stable seasonality and the existence of a non-stochastic trend element. Nonetheless, this method is both easy to implement in many common statistical packages and is relatively easily interpreble given that there is an explicit decomposition in the method between the seasonal, trend-cycle, and irregular elements. This makes it easy to use and to extend. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
