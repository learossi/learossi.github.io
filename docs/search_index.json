[["preface.html", "Personal Projects Preface", " Personal Projects 2023-04-03 Preface Hey all! These are a few portfolio projects I’ve done that are relatively quick reads just to show off what I know/can do. Here’s a quick guide the all the projects which you can navigate to using the sidebar: Confidence Interval Estimation: A statistical and simualation argument that most confidence intervals created using current research methods are too narrow. M Estimators: An intuitive explanation of different types of proofs and properties of an important statistical estimator class. Gaussian Processes: A look at how efficient Gaussian Process hyperparameter search is for random forests. Campaign Finance: An attempt to estimate the effect that money raised by candidates has on campaign outcomes independent of candidate popularity or affability. Electricity Demand: A comparison of commonly recommended neural network structures for predicting short-term domestic electricity demand. Seed Market Competition: My bachelors thesis in economics which analyzes the historical effect of increasing concentration in the seed market on farmer’s outcomes. Chicago Taxi Market: Forecasting demand in the Chicago Taxi market using a variety of time-series methods. .MathJax { font-size: 100% !important; font-style: \"tiny\" !important; "],["almost-all-regression-confidence-intervals-are-underestimated.html", "1 (Almost) All regression confidence intervals are underestimated 1.1 Introduction 1.2 Theory 1.3 Practically Speaking", " 1 (Almost) All regression confidence intervals are underestimated 1.1 Introduction So….frequentist statistics are a thing. And a very popular thing at that. The common language for most (if not all) of the social sciences is regression analysis; unfortunately, there is a lot of baggage there. The common pipeline for analysis goes as follows: determine some relationship to study, gather the IV and DV of relevance as well as a variety of control models, fit a tower of increasingly complicated models (more variables, fixed effects, interactions, etc) that control for some potential confounding factor, report some of these models, and then compare the coefficient values with your stated hypothesis and choose a confidence level at which you reject or accept your hypothesis. Almost all published research in fields like social psychology, economics, and the like are done using this model. The problem is that baked into the model are assumptions about the ways the models are estimated which don’t accurately reflect the actual way in which research is conducted. The discrepancy between the two causes confidence intervals to be too narrow and an abundance of false hypotheses that should be rejected to instead be accepted. This article is going to discuss 1) the underlying assumptions in least squares regression and how they differ from actual research practice, 2) offer a statistical argument for why this discrepancy leads to errors, and 3) show simulations which demonstrate these errors in action. 1.2 Theory 1.2.1 We run too many regressions It is common practice in many papers to estimate a lot of regressions with a lot of different parameters and then to report some subset of these. Ideally, a researcher creates a theoretical model which creates some sort of dynamical equations which imply a relationship between two variables. They then want to test if this relationship is well founded by testing that relationship using regression analysis. Ideally, they collect data on these two variables as well as other covariates (which should also be in their model) and design a regression which represents the types of dynamics they should see in their model (ie log transforms for multiplicative relationships, dnd specifications or the like for confounders). Ideally, the model they specify is actually a quite good approximation of the real data generating process. In this ideal world, the researcher would actually be justified in taking their regression, looking at the confidence interval on the relevant IV and rejected or accepting the null hypothesis. The obvious problem is that neither we, nor any researcher I’ve ever heard of, actually live in this ideal world. Frequentist confidence intervals are statistical tests which are derived from the simple idea that if we take independent samples from the same population we can set a rejection threshold on the null hypothesis which ensures we reject the null hypothesis a set number of times in 100 if it is actually true. The crucial caveat in this line of reasoning is that the samples must be independent in order for the confidence intervals to be meaningful when compared across multiple models. Let us take an illustrative example. Imagine I have a cornucopia of potential models I can fit corresponding to a plethora of IVs and their transforms, splines, interactions, etc. Imagine then that I fit 10 models and am unable to reject the null hypothesis at a 5% level for all 10 of them. Fitting an 11th model where I’m able to reject the null hypothesis should probably not lead me to the conclusion that I’ve actually found the right model and I was right all along about the underlying inference I was trying to draw. A couple of things are worth pointing out here. For one, the inferential procedure where one fits a lot of models and looks at the confidence intervals to determine the worth of the next model violates the conditionality principle, so perhaps one ought to be skeptical of the last paragraph. That said, we’re pretending we’re all frequentists here, so who really cares? Secondly, the analysis in the last paragraph isn’t remotely controversial; everyone knows this. Why the fuss then? To spoil a bit, I’m going to argue that it is almost statistically guaranteed we’ll observe these erroneous rejections of the null hypothesis and that the procedures researchers use to select models do basically nothing to guard against them. 1.2.2 Preregistration is a good thing actually Preregistration as a practice is something that has really caught on in a few fields - mostly the medical ones. The idea is that publishing a document with intending methods and data collection practices prior to actually doing the analysis forces researchers to cohere to practices that make their analyses valid. The downside is that it allows researchers less flexibility. The problems I outline wouldn’t exist in a world where preregistration is the norm: researchers would outline a group of regressions based on their theoretical study and then do them on the data and report the results. Some say this might be overly restrictive. The argument against preregistration is that data is often unpredictable and if we actually knew what kind of relationships were in the data we wouldn’t need to do the analysis. Preregistration prevents researchers from being flexible upon receiving the data and finding non obvious relationships. Of course, this counterargument assumes a best case sort of scenario. The other hand is that greater researcher freedom leads to a greater risk of manipulation (p-hacking seems to be the most common but outright fraud isn’t unheard of). So, if we were people setting policy we would like to choose some level of prior restraint on researchers that minimizes the flexibility of bad actors to distort proper inferences. The problem is that there doesn’t seem to be a good way to distinguish these two types of behaviors. Take a really canonical case of p-hacking. A researcher runs a naive regression and gets an adverse result. Not taking kindly to this, they then comb through the data to find nonconforming pieces of data they could justifiably remove, fit a bunch of models, and then report ones that agree with their hypothesis. No one disagrees this is bad. The problem is that it is quite hard to spot since there is a lot of subjectivity in data analysis. Want to exclude an observation? There are a lot of different outlier criteria. Want to justify a different model? There are a million and one potential effects that would justify the inclusion of a new term in the model. And so forth. Now, how would we characterize actual good faith inquiry into the dataset? The way I imagine it is that a good faith researcher goes in and combs through the data and does a lot of exploratory analysis and tries to look for relationships that they haven’t conjectured. When/if they find one, they fit different models trying to tease out the nuances and then go back to their theoretical model and try to conjecture some new constraint or quirk that would generate the perceived variation and treat that as a new and improved model about how the world actually works. Note, the good researcher is essentially doing nothing different from the p-hacking researcher; they take the same steps in the same order. The intention is the only thing that’s different. It is thus very hard to distinguish good from bad researchers just on the basis of their analysis. It may be that they make different decisions on the same data due to their mindset and we can differentiate them thusly. Approaches like this probably work in the most egregious cases but the aforementioned subjectivity in analysis makes this approach fraught. In fact, it’s easy to imagine a world where good intentioned researchers simply are too convinced of their own theory and engage in what would be considered p-hacking behavior despite having a good mindset. The point is that from the perspective of an outsider, if we allow researchers to fit a variety of models, it is exceedingly difficult to distinguish good actors from bad actors. In fact, the only way to do it may be to attempt a blind replication; and even that is unlikely to be convincing for many. If it turns out that it is oftentimes the case that fitting a variety of models leads to bad results, this is a real problem. 1.2.3 A statistical argument For all regression models in the limit, the inclusion of a variable which is independent from all others so far included in the model doesn’t influence the coefficients. It is easy to see this by just looking as a matrix specification as follows: \\[\\begin{equation}(X^TX)*\\beta = X^Ty\\end{equation}\\] For independent variables, the Gram matrix (\\(X^TX\\)) is diagonal and the addition of another variable on the bottom of \\(\\beta\\) does nothing to affect the rest of the system of equations. However, this is only true in the limit. In sample situations, it may the case that the projections of independent variables onto each other as in the gram matrix is nonzero (note, this is only a sample correlation function in the case of mean zero function to generate observations). In this case, the addition of a new variable may cause changes in the coefficients of the old ones. Take the very simple example of 2 IVs. \\[\\begin{bmatrix}P_{11} &amp; P_{12}\\\\ P_{12} &amp; P_{22} \\end{bmatrix} \\begin{bmatrix} \\beta_1\\\\\\beta_2 \\end{bmatrix}= X^Ty\\] Imagine now that \\(P_1\\) is zero in the case with only 1 IV meaning that \\(y\\) and \\(X_1\\) are orthogonal. Now imagine that \\(P_{22}\\) is non-zero and \\(corr(X_1,X_2)\\) is nonzero (implying that \\(P_{12}\\) is also nonzero). Then by plugging these numbers into the matrix above we can see that in order to make the first member of \\(X^Ty\\) zero, \\(\\beta_1\\) must increase/decrease as much as \\(\\beta_2\\) decreases/increases. In this way, \\(\\beta_1\\) can be pushed up or down with sufficiently small samples just by including new variables even if it is asymptotically zero. The question then becomes how easy is it to find variables which are correlated (in a loose nonmathematical sense) both with \\(y\\) and \\(X_1\\) in the opposite direction that we want to push our coefficient? This turns out to be a very challenging question to answer with any sort of generality. The nice thing is that if we simplify a bit the question of whether \\(X_1\\) is correlated with \\(X_2\\) and whether \\(X_1\\) is correlated with \\(y\\) are the same question of establishing the distribution of the sample correlation between independent variables. Let’s take the example of two groups of iid standard normal distributions \\(X_n\\) and \\(Y_n\\). When I say ‘correlated’ in the last few paragraphs, what I actually mean is that the sequences of the two variables have a nonzero dot product when treated as a vector (as in the right side of the regression equation). So, what we want to study is first \\(X_nY_n\\) then \\(\\sum_{n}X_nY_n\\). In the case of the normal distribution, the distribution function takes the form of a kind of multiplicative convolution: \\(p(x)=\\int_{-\\infty}^{\\infty}f(\\frac{x}{y})g(y)dy\\) where f and g are the two densities. Just like the case of convolutions, there is no general closed form solution to these sorts of problems (and this is even more difficult as we don’t have Fourier transforms to help us simplify the problems). It turns out that you can write \\(XY\\) as \\(\\frac{1}{4}(X+Y)^2-\\frac{1}{4}(X-Y)^2\\) which is the sum of two chi-squared variables (by the fact that independent normals are closed under summation). This fact implies we can write the characteristic function as the product of the characteristic function of the two chi-squared variables. Doing this gives us a complicated expression with the product of exponentials and some sqrt of t functions in the denominator. Perhaps there’s some hope using some sort of contour integration approach to take the fourier transform and recover some useful information about the density function(though the fact that the exponentials are of opposite sign makes this more difficult). I haven’t really tried mostly because if I succeeded it wouldn’t provide any sort of hope in the general case. Fortunately, we can take a step back and employ some simpler math to get some heuristics to guide further inquiry. Notice that 1) \\(X_1Y_1,X_2Y_2,...,X_nY_n\\) are all jointly independent by definition and 2) we’re actually interested in the sum of these since it’s the dot product which shows up in regression formula. Imagine now that we restrict our interest to variables whose generating function has finite moments. The Central Limit theorem is then an obvious tool here that allows us to examine the limit distribution. For independent \\(Y\\) and \\(X\\), \\(E[XY] = E[X]E[Y]\\) so we can use information about the individual distributions to determine the mean. I assume wlog that one of the distributions is mean 0. Thus, \\(\\frac{\\sum_NX_nY_n}{\\sqrt{N}\\sigma(XY)} \\rightarrow N(0,1)\\). Note that the \\(\\sqrt{N}\\) factor is in the denominator because the sum of the product over \\(N\\) is a sum and not a sample mean. The implication is that \\(\\frac{\\sum_NX_nY_n}{\\sigma(XY)} \\rightarrow N(0,N)\\), or, in english, the variance approaches infinity at a linear rate as the sample size increases. This is good as we want the eigenvalues of the gram matrix to become arbitrarily large for uncorrelated samples as \\(N\\) increases as this allows the betas in the regression equation to become arbitrarily small. Of course, the probability of the sum being within any compact set never goes to zero, but this isn’t a problem as it becomes arbitrarily small (literally the opposite of bounded in probability) which is good enough for our purposes. The fact that we have the correct asymptotic behavior is comforting but it only tells us some of what we need to know in the finite sample realm. Here, once again, we can only speak in broad generalities since rates of convergence to the normal approximation depend on the distribution. Roughly speaking, the smaller the larger the moments &gt;2, the slower the convergence. This can be seen by looking at the Taylor expansion in the moment generating function proof for the CLT. Given that we allow all distributions with finite moments, there isn’t a good procedure to create upper bounds on the rate of convergence. Fortunately, this is the exact same problem that researchers run into when estimating confidence intervals in normal regressions and they come up with variable transformations and the like that make them comfortable the approximation is sufficient so I don’t know why I shouldn’t also be. As long as we accept this, it also shouldn’t provide us much discomfort that we had to restrict ourselves to variables who are generated with finite moments as we believe in all practical cases we believe we can transform unbounded variables to bounded ones. All this is to say that the normal approximation to the limit distribution is probably a good one. From a theoretical perspective this is nice for me, but I now argue that it should make us less confident in the regression analyses that we read. The basic argument is as follows: given the linear increase in the variance from the normal approximation, it takes a lot more samples if we want to make it substantially harder to find a variable transformation which leads to a significant coefficient on the operative variable. Conversely, the number of possible regressions grows exponentially. The number of regression equations possible with n IVs is \\(2^{n-1}\\). We get that again if we include all squared terms. The number of possible interactions is \\(2^{n^2-n}\\). The point is the number of possible regression equations grows quite fast if we allow commonly used transforms. The way to think about this conceptually is that we want to find transformations of the generated samples which point in certain directions such that the resulting regression equation yields a significant value for the IV contained in the hypothesis (it’s actually an affine subspace we’re searching for rather than a direction, but same difference). If the allowable transformations sufficiently mix the space, then the exponential growth of allowable regressions functions means that the probability of finding one of those directions becomes quite high. There is more math that could be gone into here, but approximation and ergodic theories are their own worlds and I haven’t been particularly brief as it stands. That said, we should expect intuitively that functions like power functions provide the proper mixing properties given the completeness of the Taylor series for approximating analytic functions. To test this, I run a few simulations. 1.3 Practically Speaking 1.3.1 Simulations I would love to run simulations with a lot of samples and variables and so forth, but my laptops bones are old and weary, and thus won’t allow me to. So, I’ve settle for something a bit more modest. The idea behind the simulations is simple: I generate random normal variables equal to the number of IVs plus one (corresponding to the DV). The first IV is the one whose coefficient we’d like to test if we can make it significant and will be called the ‘hypothesis variable.’ All the other are ‘control variables.’ The mean and standard deviation of each variable are controlled using hyperparameters and are thus standard normal. I run simulations using relatively conservative constraints. For the first experiment, I take all control variables, all interactions between, and all power of 2, and then run a regression over all subsets of fixed sizes that don’t include any of the basis function transformations of the hypothesis variables (in this case, all subsets of four linear terms, two interaction terms, and three polynomial terms). I then repeat this over a large number of samples and report the measured quantiles for the best case p-values on the hypothesis variable. To interpret the graph above: the red is what one would expect if the confidence interval estimates were actually proper (ie we would expect to see a p-value less than .2 twenty percent of the time and so forth). The blue values is what we actually observe. Just looking at it roughly, it seems the p-values which are calculated the normal way are around 30% more confident than they really ought to be. This is a pretty big disparity; that said, the fact that the disparity is substantially lower at the low p-values is somewhat heartening. This simualtion is only a very small fraction of the broad expanse of models we could reasonably expect a researcher in this situation to estimate though. So, let’s broaden things up a bit. The next simulation is the same as the last except that transformations of the hypothesis variable are allowed and we report a positive result if any of these transformations yields a positive p-value. The results are as follows: This is much much worse. For almost all samples (which should be completely independent) we can find a transformation that gives the hypothesis variable a p-value of .2 or above. For about a quarter of samples we can find one that is significant at the 1% level. Clearly the conventional p-values are way off here. And we this is still a severely restricted set of transformations that we’re working with. Not good. 1.3.2 What does it all mean? It is worth noting that the problem of it being too easy to fit models to data is in no way exclusive to academic fields that gravitate towards regression analysis or even fields where statistics are all that prominent. Particle physics has a similar sort of problem. After the failure of the LHC to find supersymmetric particles, a common accusation lobbed against string theorists is that whenever there is an adverse experimental finding against their theory they always adjust the theory by adding a new particle or whatever so that it against conforms with experimental results without grappling with the failure to make accurate predictions in the first place. The string theorists generally respond that every theoretical framework has falsifiable versions of it and that the underlying unification of gravity and quantum mechanics is quite valuable1. One can come down on either side of this debate but the point is that these sorts of issues aren’t restricted to the softer sciences. The larger point is that there are very difficult systemic issues in how we do research that make it hard to get reliable results. I’ve talked a lot about p-hacking vs honest inquiry in the previous sections and that’s an important framing device for why we care that confidence intervals are probably too wide. That said, I think these issues would still persist even if we were totally confident that all researcher were totally honest and none were p-hacking. Researchers are simply given too much flexibility to create models. If we frame the problem like this, the only solution is to restrict the flexibility researchers have. Broadly, I think I’m in favor of this; however, there are good and bad ways to accomplish that goal. I’ve already alluded to pre-registration being a good paradigm to use. In a purely Platonic sense, the restrictions to researchers there are rather severe in the sense that researchers can’t deviate much from the plan2. The key side benefit of this is that it forces them to think more carefully about the theoretical underpinnings of the types of models they’re building and why they are making the assumptions they do. This is a really really good thing as in-depth model analysis is something that just doesn’t really happen in the empirical literature in fields like economics. Another good path would be to encourage researchers to make it easy to replicate their work. As it stands, in many papers the control variable matrices often aren’t even fully specified as to what variables they contain. It’s hard to check if these controls are reasonable as very few authors give the data and scripts necessary to replicate their work. Such access could serve as a sort of check on bad research that abuses the data. Of course, what this really boils down to is just the idea that people should be better at research. People should use better methods, should build better models, etc. And in some respect that seems like wildly wishful thinking in a sense that I agree with. The problem is that the world we live in is one where there’s a lot of bad research and fewer institutions to correct that than there should be. So, if the world can’t be perfect, at least we can try some things that might make it a little better. Entertainingly, physics also has a statistical problem where they set confidence intervals for their measurements of fundamental constants and pretty regularly get updated estimates that are outside those intervals from more precise experiments, but that’s neither here nor there.↩︎ Or they can’t deviate much in the platonic ideal case of pre-registration. Practical implementations often have researchers giving much less information than might be desired.↩︎ "],["convergence-of-m-estimators-made-simpler.html", "2 Convergence of M-Estimators made simple(r) 2.1 What is an M-Estimator? 2.2 Consistency 2.3 Rate of Convergence", " 2 Convergence of M-Estimators made simple(r) I’ve been reading Van der Vaart (2000) recently and, to be frank, I’ve found parts of it confusing. Chapter 5 deals with the properties of M and Z estimators and is the bedrock for many portions of the book in that most types of statistics can be studied in the context of these sorts of estimators (sufficient conditions for the consistency of maximum likelihood are often expressed as M estimators). Unfortunately, as is very common in statistics, the estimator properties are highly polymorphic in different types of spaces and with different constraints on the estimators themselves or on their limits. This reality necessitates a variety of theorems that I find obscures the important overriding themes about the relevant convergence theorems. Instead of simply repeating the proofs in Van der Vaart, I aim here to more clearly explain the important properties of such estimators and the implications of the proofs. Detailed explanations of the proofs can be found in the book itself since I don’t really see the point of going line-by-line again here. 2.1 What is an M-Estimator? M-Estimators are important because their definition is sufficiently general to include most estimators in common use. The most important is obviously maximum likelihood (whose properties are ironically usually proved using its correspondence to method-of-moments but that’s neither here nor there). The main problem M-Estimators try to solve is as such: we have a collection of iid random variables who come from one in a family of distributions indexed by a parameter \\(\\theta\\): find a guess for \\(\\theta\\) that converges to the population value as number of realizations of your random variables increases to infinity (the estimatory is more general than that but the previous problem is usually the one it solves). M-Estimators consist of a set of iid random variables, \\(X_i\\), and a sequence of functions \\(M_n\\) that defines the estimator. For each realization of n random variables, the M-Estimator is the value of \\(\\theta\\) which maximizes the function \\(M_n\\). The most common form of an M-estimator is as a sum of n criterion functions \\(m_n\\): \\[ \\begin{equation} \\operatorname*{argmax}_\\theta M_n = \\operatorname*{argmax}_\\theta ( \\frac{1}{N}\\sum_{N} m_\\theta(X_i)) \\end{equation} \\] A closely related concept is the Z-estimator (my guess is that it’s Z as in “zero-estimator” or of one the usual leftover fascinations of mathematicians with the German language). Z-estimators are defined similarly except they replace the argmax with an equality condition. The corresponding form with criterion functions is: \\[ \\Psi_n(\\theta) = \\frac{1}{N}\\sum_{N}\\psi_\\theta(X_i)=0 \\] The Z-estimator is the value of \\(\\theta\\) which satisfies the equality. These two estimators are equivalent for differentiable functions where \\(\\psi_\\theta\\) is just the derivative of \\(m_\\theta\\). Even if the function is non-differentiable, similar regularity conditions can be applied to maxima and zeros respectively which allows consistency proofs for one type of estimator to be applied to the other. The choice of \\(m_\\theta\\) allows the estimator to function differently depending on what you want \\(\\theta\\) to estimate. Choosing \\(m_\\theta\\) to be the log-likelihood recovers the maximum likelihood estimator. A Z-estimator for \\(\\sum_{i=1}^{n}(X_i - \\theta)\\) recovers the sample mean. One can also choose more general sequences of functions \\(M_n\\) though this necessitates a separate convergence proof. This flexibility means many types of estimators can be recast in this light which allows for simpler consistency and convergence proofs. 2.2 Consistency In the case where an M-estimator is the argmax of a sum of criterion functions, the value of \\(\\lim\\limits_{n \\to \\infty} \\sup_{\\theta} M_n\\) is trivially equal to \\(E_X(M({\\theta_0}))\\) where \\(\\theta_0\\) is the true parameter that maximizes the function across the true distribution function. This follows by the law of large numbers and is essentially definitional. The following proofs assume the property that \\(M_n(\\theta)\\) converges to the maximum: more precisely, for the sequence of estimators \\(\\theta_n\\), \\(M_n(\\theta_n) &gt; M(\\theta_0) - o(1)\\) where \\(o(1)\\) is a function that converges to 0. For more complicated \\(M\\)s which don’t make use of sums of criterion functions, this property must be proved separately of the consistency proof which makes them much more unwieldy. Rather than diving into the few consistency proofs, it might merit dicussion what exactly is it that can cause an M-estimator to not be consistent. We’ve already assumed that the output (separate of the value of \\(\\theta\\)) of the M-estimator converges to the maximum across the parameter set: however, this is not the same as consistency. To be a consistent estimator, the M-estimator has to converge to a single \\(\\theta\\) which has the desired function value. There are two separate cases where this can fail. For one, if \\(M_n \\rightarrow M\\) st \\(M\\) has multiple equal maxima it may be that \\(M\\) oscillates between those maxima and never settles on one to converge to even in the limit. Secondly, imagine the parameter set isn’t compact. While the maxima may occur on a defined point, it may be that the maxima occurs at infinity in the limit which corresponds to a train of maxima going on infinitely for the \\(M_n\\) which don’t even have a convergent subsequence. Consistency proofs take on different assumptions about \\(M_n\\) that try to control these pathological conditions in order to attain consistency. The most basic of consistency proofs is Wald’s consistency proof which is commonly referred to as the “classical” approach to consistency. Here, the estimator takes the criterion function form which is assumed to be upper-semicontinuous. Generally speaking, one needs some assumption about maximum of the limit estimator being unique, but Wald doesn’t assume that. So, the classical proof doesn’t rule out oscillating solutions on a set of multiple maxima and is more of a consistency in the loose hand-waving sort of way proof. Nevertheless, is does rule out divergence to infinity sorts of pathologies by assuming that the set of parameters is either compact or is eventually bounded (this may need to be a separate proof). From there, the compactness condition allows one to build a finite subcover of the parameter space except for decreasing neighborhoods surrounding the maximizing parameters. It is then shown using the convergence assumption that the probability of the parameter n being in any of those sets approaches zero. This proof is nice in that it should be familiar with anyone who has taken a real analysis course. However, the assumptions are very restrictive and hard to generalize: this is especially true of the compactness assumption. Additionally, the fact that it proves convergence to a set instead of a point is problematic in that we would like to use this to put bounds on the asymptotic convergence which we can’t do if it doesn’t technically converge. The next two proofs aim to remedy these concerns by adding new (and hopefully minimal) assumptions to introduce true converge to a broader range of solutions. The easiest way to replace compactness in the parameter set is to impose regularity on the type of allowed convergence to the asymptotic M-estimator. If we assume that \\(M_n\\) converges to \\(M\\) uniformly and the maximum of the asymptotic estimator is strictly separated (ie there is a parameter value for which all values outside a neighborhood are less) then we have consistency in a more common sense. In this case, the strict separation assumption prevents an escape to infinity since the sequence of estimators must settle on some value eventually. The uniform convergence assumption prevents highly oscillatory behavior that would create multiple points of convergence like in the Wald proof. Specifically, the uniform convergence assumption allows us to establish that \\(M_n\\) converges in probability to \\(M\\) which is much stronger than the Wald result. 2.3 Rate of Convergence Rate of convergence proofs are difficult since M-estimators can be any arbitrary estimator as long as it fits the convergence criteria. The criterion function formulation of M-estimators is popular as this is one area where we can guarantee a radical rate of convergence given some regularity conditions (discussed below). One might expect that the criterion function formulation has the canonical \\(\\sqrt n\\) rate of convergence given that the estimator is the sample mean of the estimator results. However, each of the estimator outputs is actually taken over the product space of all the random variable realizations. Therefore, the \\(M_n\\)s aren’t independent and the CLT doesn’t apply. The further problem is that M-estimators need not take this criterion function form and can be more pathological. It is easy to construct a consistent M-estimator that has an arbitrarily low convergence rate as long as one is willing to throw out an arbitrarily large amount of data. Let’s do \\(e^n\\). Take a given sequence of estimators with a \\(\\sqrt n\\)-law. Use this to construct a new sequence of estimators where each of the previous estimators \\(M_n\\) is repeated \\(e^n\\) times irregardless of the outcome of the random process. This process now as a \\(e^n\\sqrt n\\) rate of convergence (it is sufficient that the values of the new norming sequence agree with the old norming sequence on one estimator for each n-length sequence in the new estimator sequence up to a multiplicative constant which is easy to check). The problem of identifying a convergence rate is therefore too general to provide overall heuristics for any type of estimator. However, there are useful theorems that provide polynomial rates of convergence for different conditions on the estimator sequence provided it is of the criterion function form. The following example focuses on relating bounds on the changes in the population mean to bounds on the sample mean: this type of analysis will probably be familiar to those well versed in the optimization, signal-processing, or machine-learning literature. The rate of convergence is controlled by a deterministic part and a stochastic part. If the models around the true parameter are very different from the true model (ie the true model is a sharp maxima), then convergence will be faster. This faster convergence is because it will be easy to pick out the actual maxima from the “noise” due to the inherent randomness of the sampling procedure. Dually, if the sample mean converges to the true mean quickly, that level of “noise” is lower and overall convergence is therefore faster. A more formal statement is suppose that \\(M\\) is of the criterion function form and has bounds \\(\\alpha\\) and \\(\\beta\\) where \\(\\alpha\\) is essentially the bound on the convergence rate of the criterion function relative to its parameter and \\(\\beta\\) is the bound on the expected deviation of the sample mean from the population mean for the criterion function (see book for a more rigorous formulation). The rate of convergence is then \\(n^\\frac{1}{2\\alpha-2\\beta}\\). This can be applied to Lipschitz functions to give the usual \\(\\sqrt n\\) rate of convergence. References "],["gaussian-process-mbo-for-random-forests.html", "3 Gaussian Process MBO for Random Forests 3.1 Overview of Gaussian Processes 3.2 Application to Random Forests 3.3 Data 3.4 Results 3.5 Discussion", " 3 Gaussian Process MBO for Random Forests 3.1 Overview of Gaussian Processes Hyperparameter tuning is a tricky subject. Oftentimes, one simply conjures up a reasonable value from the some article about whatever architecture that one read sometime or another and plays with that until it works. This type of approach is reasonable insofar as hyperparameter tuning is very, very hard and there are a lot of answers that may look, a priori, equally reasonable while providing wildly different results in practice. Take neural networks for example. One could take a given value for the learning rate. Except that there’s a lot of literature that shows learning rate and batch size interact to shape the loss the function and one can’t set them alone. Or say you want to vary the weights of different hidden layers. Except that also affects your weight initialization strategy. And it goes on. Hyperparameter optimization is often very difficult and consensus on good decisions for any one dataset (think MNIST) is frequently the product of a multitude of separate experiments. The go-to approach when one has a relatively flat prior about the efficacy of a set of parameters within a compact set is just grid search. This does get the job done but is computationally expensive and ruinously vulnerable to the curse of dimensionality when dealing with complex or highly tunable models. All these facts make efficient automated approaches to hyperparameter tuning highly desirable. The issue here is that it is often impossible (or unclear how) to differentiate model parameters with respect to the data. Either the models employ functions which themselves are discontinuous or nondifferentiable or the parameter values are integers which means there is no reasonable interpretation of the change in objective in a neighborhood around the parameter. The lack of a derivative means that most traditional continuous optimization methods become either useless or inefficient (integer programming for example). There are a few possible solutions to this problem. One is to transform the model so that it may be differentiated. VAEs use a parametric assumption on latent space to allow differentiation. Random hinge forests are a differentiable version of the random forests I will talk about later. Obviously, differentiability is not always possible (but always imposes significant additional complexity on the model). Therefore, a more flexible parameter search option is desirable. One way to do this is to model the hyperparameters and the resulting loss as a gaussian process. Gaussian processes are any collection of random variables where any finite subset thereof are distributed jointly gaussian with an arbitrarily flexible covariance matrix. In this instance, the random variables are the objective values of the function. Gaussian processes generally assume that all variables have an unconditional mean of zero; however, they may also have a non-zero conditional mean depending on the realizations of other random variables and the relevant covariances. Estimation procedures for gaussian processes exploit this fact to build flexible models of the objective function. The main procedure is as follows: one obtains a requisite number of samples of one’s objective function with differing hyperparameter values. One then chooses a stylized covariance matrix in the form of a kernel function. Technically speaking, nothing about the definition of gaussian processes above implies they are continuous (ie they might have negative or zero covariance locally). However, continuity is a property we expect from basically all our objective functions. To ensure the estimate of the objective function satisfies this continuity requirement, covariances are assumed to a kernel function of the metric of the distance between the two functions. This assumption creates a parametric framework for the covariance which otherwise could not be estimated, imposes continuity on the function, and still allows the process to be a universal approximator given sufficient sampling. With the covariance assumption in place, estimating the value of a point in the hyperparameter domain is as simple as calculating the conditional mean of a jointly gaussian variable. Of course this is just an estimate of the actual value of the objective function and may bear only a small resemblance to reality. The procedure to find the actual minima in the hyperparameter surface is an iterative one. Given the sampled points, some nonlinear optimization is used to find an estimate of the minima of the initial objective function; BSGF is the most common method but others are obviously available. The estimated minima is then sampled and included with the other points to form the basis for a new objective. This process is then repeated until some stopping rule is reached. The primary benefit here is that the number of models trained is minimized. The tradeoff is that one must pay for the cost of whatever nonlinear optimization function one is using. Whether that is beneficial largely depends on the hyperparameter space and the model among other things. 3.2 Application to Random Forests I’m going to test this on random forests. Random forests are a bagging method that allows multiple (relatively) uncorrelated trees to vote on the class of an observation. In short, multiple decision trees are fitted to individual bootstrap estimates of the sample data. When splitting each tree at each level, features are again bootstrapped to reduce tree correlation. An observation is then given a predicted class value based on the plurality of trees. Random forests are beneficial in that there are a lot of potential hyperparameters to tune. One can adjust the number of trees, maximum length of trees, the maximum number of terminal nodes, the bagging parameters for the individual trees, the bagging parameters for the nodes, and so forth. The hyperparameter space can be high dimensional if one chooses to make it so which makes it ideal for testing out gaussian process regression. Random forests are also traditionally very flexible which make overfitting an issue. So, instead of tuning the training error as the objective function, I use the test error in n-fold cross validation as the objective function. 3.3 Data I use a dataset of wine vintages from the UC Irvine machine learning repository. It has approximately 180 wines from 3 different italian wineries. Each of them were given chemical composition tests that give values for a variety of quantities like alcohol percent, hue, or the amount of phenols. There are 13 variables in total and which are split roughly equally between three possible classes. plot_data = melt(select(wine_data, !c(proline, origin)), values.name = &quot;Value&quot;) %&gt;% mutate(value = log(value)) ## No id variables; using all as measure variables ggplot(data = plot_data, aes(variable, value)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + labs(title = &quot;Variable Distributions&quot;, x = &quot;Predictive Variables&quot;, y = &quot;log(Value)&quot;) I take the log for this graph to make each level visible on the same graph but don’t do so for the data itself. Anyways, most variables have roughly the same variance, lack distinguishable tails, and have roughly similar levels. It’s nice data where I don’t have to do much (databases are so nice). The amount of data is relatively small but that’s good for me because it is still able to be used while not requiring me to run grid search on a massive dataset. The implications thereof will be discussed later. 3.4 Results 3.4.1 Grid Search I use the RandomForest package from R to do this analysis. It has a wide variety of parameters over which I can optimize and I select a few. The first is the number of trees that the random forest produces. This parameter controls the flexibility of the model; the more trees there are and the more bagged samples the trees are being trained on, the more specific models will be able to specialize on certain parts of the data. The next variable is the number of bagged variables that are sampled at each split; a lower number increases tree decorrelation but raises the number of trees required to represent the sample because each tree is less powerful. There is also a binary variable that controls if sampling with replacement is done that I allow to be toggled along with a variable that selects the sample size for each tree. And finally, the node size parameter controls how many samples can be contained within a node without it splitting. Together, these form the basis of a parameter set in the ParamHelpers pack that we can plug into the mlrMBO package to optimize it. The main metric I’m interested in is how many iterations it takes the gaussian process to obtain a solution with a comparable objective value as the one that the grid search obtains. Given that I’m concerned about speed, I could hypothetically time each method and report those speeds. However, that seems besides the point to me. As mentioned, the main factor affecting speed is the time it takes to train the model and the convergence rate of the optimization algorithm. If I found that grid search was faster, I could replace this model with a similar one that takes 10x longer to train and get the opposite result. The convergence speed of the gaussian process is something that is less subject to changes in the previous two variables and thus more interesting an object of study. Anyways, cross validation libraries in R tend to be a bit library dependent and it’s pretty easy to implement, so I write my own k-fold cross validation script. Instead of explicitly defining parameter values, forest_cv = function(n_folds,predictors, outcomes, forest_vars){ loss_per_fold = c() data_left = nrow(predictors) per_fold = data_left/n_folds empirical_losses = c() shuffle = sample(nrow(predictors)) pred = predictors[shuffle,] out = outcomes[shuffle] fold_train_in = data.frame() fold_train_out = data.frame() fold_test_in = data.frame() fold_test_out = data.frame() for(i in 1:n_folds){ if(data_left &lt; per_fold){ fold_train_in = pred[-(((i-1)*ceiling(per_fold)):nrow(pred)),] fold_train_out = out[-(((i-1)*ceiling(per_fold)):nrow(pred))] fold_test_in = pred[((i-1)*ceiling(per_fold)):nrow(pred),] fold_test_out = out[((i-1)*ceiling(per_fold)):nrow(pred)] }else{ fold_train_in = pred[-(((i-1)*ceiling(per_fold)):(i*ceiling(per_fold))),] fold_train_out = out[-(((i-1)*ceiling(per_fold)):(i*ceiling(per_fold)))] fold_test_in = pred[((i-1)*ceiling(per_fold)):(i*ceiling(per_fold)),] fold_test_out = out[((i-1)*ceiling(per_fold)):(i*ceiling(per_fold))] data_left = data_left - ceiling(per_fold) } forest = do.call(randomForest, c(list(x = fold_train_in, y = fold_train_out), as.list(forest_vars))) loss = data.frame(preds = predict(forest, fold_test_in), vals = fold_test_out) loss = loss %&gt;% transmute(loss_vals = case_when(preds == as.numeric(vals) ~ 0, TRUE ~ 1)) %&gt;% dplyr::pull(loss_vals) loss_per_fold = c(loss_per_fold, mean(loss)) } return(loss_per_fold) } The forest_cv function accepts a list or vector of named parameters to randomForest as input. To generate these, I create lists of each acceptable param value and just use what is effectively a cross join to create all possible combos (filter can be used afterwards if you have conditional dependencies between variables, even though I don’t). grid_param = list(ntree = seq(from = 1, to = 50, by = 2), mtry = seq(from = 1, to = 12, by = 1), nodesize = seq(from = 1, to = 10, by = 1), sampsize = seq(from = 80, to = 180, by = 5), replace = c(TRUE, FALSE)) %&gt;% expand.grid() Plugging these parameter values into the forest_cv function gives me a validation set loss for each set of parameters. Predictably, this takes a long while to run. Notice that, because training trees is stochastic, loss values can vary across each run. The large number of parameters means that it’s actually relatively difficult to search for a single configuration of parameters that is superior to any other. In fact, there were 9 of the parameter configurations which achieve zero error even on the validation set. filter(grid_loss, grid_loss == 0) %&gt;% select(!c(test_list, X)) ## grid_loss ntree mtry nodesize sampsize replace ## 1 0 43 1 8 95 TRUE ## 2 0 9 5 4 70 FALSE ## 3 0 17 1 1 95 FALSE ## 4 0 25 2 1 105 FALSE ## 5 0 9 2 4 110 FALSE ## 6 0 13 1 7 115 FALSE ## 7 0 25 2 6 120 FALSE ## 8 0 25 3 10 120 FALSE Obviously, these values are suggestive in that certain parameters seem to bunch around certain values. Ntree looks like it might be optimally 25, mtry 3, samplesize is mostly in the higher part of the possible range, replacement increases the variance too much, etc. However, any of those conclusions require a degree more of interpretation than would perhaps be desirable for a process which is supposed to be more automatic than other hyperparameter selection methods. The presence of a stochastic fitting method and a lot of grid search points - 66k to be exact - means there’s a very real threat that the grid search overfits to the validation set. We’ll return to the general question of test error later on, but one way to evaluate this is to look at broader plots of error across the range different variables like the one below: scatter_data = filter(grid_loss, mtry == 3, nodesize == 10, replace == FALSE)[,c(&quot;ntree&quot;, &quot;sampsize&quot;, &quot;grid_loss&quot;)] sc = scatterplot3d(scatter_data, pch = 16, color = &quot;steelblue&quot;, type = &quot;h&quot;, main = &quot;Validation Loss on 2 Parameters&quot;, xlab = &quot;# Trees&quot;, ylab = &quot;Bagged Sample Num&quot;, zlab = &quot;Loss&quot;) my.lm &lt;- lm(scatter_data$grid_loss ~ scatter_data$ntree + scatter_data$sampsize) sc$plane3d(my.lm) Looking at the points visually (and at the regression plane) seem to confirm the intuition the greater number of trees and larger samples are good. However, it is easy to look at any region in the plot and find points which have low error. Of course, this doesn’t mean grid search has hopelessly overfit per se. Just that one can find evidence that many configurations of hyperparameters are efficient if one wants to and that a more judicious procedure is necessary for selecting the final grid point. 3.4.2 Gaussian Process Search I use the mlrMBO package for Gaussian Process search. This package wraps the ParamHelper, smoof, and DiceKriging packages which implement most of the necessary logic. The MBO function accepts 3 types of objects: a learner which determines the next step to take in the parameter space, a design matrix which gives the priors over the space, and a control object which determines termination and local search depth. In my case, I sample 20 points uniformly from the parameter space, and allow a 10 iteration focus search to identify a locally optimal point. I allow the process to run for four iterations. forest_obj = smoof::makeSingleObjectiveFunction(name = &quot;Gaussian Process Forest&quot;, fn = forest_wrapper, has.simple.signature = FALSE, par.set = makeParamSet( makeIntegerParam(id = &quot;ntree&quot;, lower = 1, upper = 50), makeIntegerParam(id = &quot;mtry&quot;, lower = 1, upper = 12), makeIntegerParam(id = &quot;nodesize&quot;, lower = 1, upper = 10), makeIntegerParam(id = &quot;sampsize&quot;, lower = 70, upper = 120)) ) ctrl = makeMBOControl(propose.points = 1) %&gt;% setMBOControlTermination(iters = 5) %&gt;% setMBOControlInfill(crit = crit.ei, opt.focussearch.points = 10) des = generateDesign(n = 20, par.set = getParamSet(forest_obj), fun = lhs::randomLHS) lrn = makeLearner(cl = &quot;regr.km&quot;, predict.type = &quot;se&quot;, covtype = &quot;gauss&quot;) mbo(fun = forest_obj, design = des, control = ctrl, learner = lrn, show.info = TRUE) This code block is going to output an absolutely huge amount of data about the optimization path which is frankly rather hard to read and completely irrelevant. Instead, I’m going to copy over literally the only line of the output which actually matters: ## Recommended parameters: ## ntree=31; mtry=3; nodesize=6; sampsize=99 ## Objective: y = 0.018 This result seems very reasonable based on our previous investigation of what optimal parameters might be. Perhaps the number of trees is too large or sample size is too small, but the values are close. The validation loss (“Objective” in the output) is higher than comparable objectives in the previous section, but this could be expected given the previous discussion on overfitting. Notice that there isn’t really that much interpretation for me to be doing here: I run the algorithm and the fact that I’m doing many less function evaluations means that there’s no huge space of results I need to consider. I just pick the one with the lowest loss. 3.5 Discussion The most notable thing here is that I have gotten a similar result to grid search with the gaussian process only by doing 4 iterations that evaluate a maximum of 10 points each. This is a miniscule fraction of the 66k evaluations that I did in the grid search. I know I said I wouldn’t talk about time, but it is worth mentioning that the grid search took 45ish minutes while the gaussian process took easily less than a minute. The easy conclusion is that gaussian processes are a lot more efficient at finding approximate solutions that are near the minima but might not be the best compared to an exhaustive search. However, this analysis neglects the overfitting concerns that imply that the lower bounds on the validation error for exhaustive type searchs are misrepresentative of the true test error. I’ve held out a test set of 20 observations; let’s take a look at the test errors there compared to the test errors on the recommended model from gaussian process regression. grid_loss %&gt;% filter(grid_loss == 0) %&gt;% rename(`Test_Loss` = test_list) %&gt;% select(!c(grid_loss, replace, X)) ## ntree mtry nodesize sampsize Test_Loss ## 1 43 1 8 95 0.00 ## 2 9 5 4 70 0.05 ## 3 17 1 1 95 0.05 ## 4 25 2 1 105 0.00 ## 5 9 2 4 110 0.00 ## 6 13 1 7 115 0.00 ## 7 25 2 6 120 0.00 ## 8 25 3 10 120 0.00 Probably unsurprisingly, these models do a good job of describing the data and most have zero test loss also. However, a couple of them do manage to misclassify an observation or two. When we run the model that the gaussian process selects we get a test error of zero which is exactly in line with what we get from the best models in grid search. There are two points to be made here. First of all, it’s hard to say exactly how trustworthy this result is because of the sample size. 20 observations isn’t very much and the power of this test isn’t very high so it could be that the gaussian search model is overrated. That said, the structure of the gaussian process which encourages fewer evaluations could also be viewed as regularizing which would have the tendency to increase its performance on test sets. Obviously it has less information about the performance on the training set than the grid search but that doesn’t necessarily imply lesser performance on the observations we care about. All this gives a pretty good account of gaussian processes as a method; I’d like to just point out a couple more things that make them desirable. For one, they adapt better to increasing dimensionality than grid search does. The amount of evaluations for grid search is exponential in the dimension of the data so it’s not like the competition is hard here. It’s very hard to put bounds on the performance of gaussian processes because it mostly depends on how well the funtion concords with the kernel but empirically convergence is relatively fast and the local search algorithms they use have polynomial bounds for sufficiently regular functions (like most loss functions). They also are able to get a better local estimate of the optimal hyperparamter arrangement than grid search. Grid search inevitably allows gaps between points in order to make the number of evaluations tractable. These gaps can become rather large (especially when the dimension is high) and naive grid search methods have no way to interpolate between them. Gaussian processes aren’t restricted this way and can narrow in on a very specific local estimate at any value allowing them to get a better outcome in the limit than grid search. I’m not going to pretend like there are all positives. Gaussian process search is rather finicky and there aren’t many packages that do it in R. The ones that do tend to all have their own specific type of objects that they like and accept and don’t play nice with others. It can get complicated and is almost definitely overkill for small projects. However, when hyperparameter searches are important and function evaluations are expensive, this is a great option that minimizes the search time and is highly tunable for your specific purpose. "],["the-effect-of-increased-campaign-spending-on-election-results.html", "4 The Effect of Increased Campaign Spending on Election Results 4.1 Background 4.2 Motivating Trends 4.3 Initial Regression Analysis 4.4 IV Regression 4.5 Discussion &amp; Conclusion", " 4 The Effect of Increased Campaign Spending on Election Results 4.1 Background Much of the discourse surrounding money in politics and campaign finance in the US make it seem like money is a large determining factor in the outcomes of elections. Representatives certainly believe this to be the case (otherwise they wouldn’t spend a large portion of their days making what, by all accounts, are mind-numbing calls to donors) and neither do operatives who make strategic choices about where to spend money (I myself have heard from one’s mouth that they are positive they can influence elections by spending more). However, the academic consensus on this question is much more open (Levitt 1994, Ansolabehere et al. 2003).One thing is relatively clear: candidates who have money do win elections at an astonishingly high rate. This has been a consistent finding since spending has been measured and yields an easy explanation: campaign spending plays a decisive role either in driving turnout or in influencing public opinion. However, this “obvious” explanation conceals an equally obvious methodological problem. Candidates who raised more money and won their election might have won their election because they indeed were able to spend more or, alternatively, they might have been able to raise more because they were a better candidate and some amount of money on the margin had little to no effect (note that, while there are plenty of low visibility candidates from small parties who raised little money and got few votes, we can only really measure marginal effects of spending on votes for high quality candidates). This causal problem creates a substantial public policy issue because it becomes difficult to ascertain to what extent money actually influences election results and thus complicates debates on if policy intervention in federal elections are appropriate. I look at house elections from 2010 to 2022 in this analysis mostly because house election are relatively homogeneous and have substantially more elections than the Senate or Presidency. State reporting of election financing is spotty and regulations involving election financing vary wildly which makes them a less appealing focus of study compared to Federal elections. 4.2 Motivating Trends The most obvious conceptual model of campaign spending is as a simultaneous-equation-model (SEM). Elected officials don’t like to fundraise and have a lot of things to do in the meantime. Challengers’ decision to fundraise trades off with other activities that can support their candidacy like engagement with the community, interviews, etc. So, incumbents observe the potential threat that the challenger poses and decides to set a fundraising based on that while candidates decide on fundraising levels based on their perceived returns and projections of incumbent spending (from a game theory perspective, this is actually an iterated game, but the SEM approximation is good enough). To identify this model, we would need exogenous variation in spending for both types of candidates. However, while conceptually pleasing, there is reasonable heuristic evidence to suggest this conceptual model isn’t entirely accurate. To illustrate this, we pivot our focus from proportions of winners to log raw money spent (total spending has a thick tail) which reveals some interesting trends: There are clearly different effects of money in different parts of this graph so I’m going to break it down section by section. Firstly, most election losers spent less than average and the amount of their spending is clearly correlated with their vote share. The most money was spent by those candidates who had vote shares around 50%. These elections are the most competitive and where we’d expect the most spending to take place because an additional $50k or so has the highest chance to change the election outcome. For those candidates who scored substantially above 50% of the vote, there appears to be no relation between vote share and amount spent. Notably, candidates in this zone generally outraised candidates who achieved less than 50% of the vote and have a much more consistent level of funding across candidates. Most of this disparity can be explained by the advantage of incumbency. Another persistent fact of American elections is that incumbents win elections: they win elections a lot. In fact much of the variation in the previous graph can be explained by whether or not incumbents won reelection that year. Redoing the previous plot with points colored by incumbency shows that almost all of the points in the upper part of the graph are incumbents. There are many possible reasons why incumbents have such a persistent advantage in fundraising. For one, incumbents are probably higher quality candidates on average than challengers given that they have greater experience and that they have won elections before which possibly drives higher individual donations. Incumbents also have higher name recognition due to their political service which they can leverage to fundraise. The reality of incumbency advantage can also drive donations to incumbents. If a company or wealthy individual wants to influence policy or buy access to politicians but has a limited budget to do so, they are most likely to funnel their donations to the candidates which are most likely to win: which are the incumbents. This reality means that money probably has very different effectiveness depending on how competitive a specific race is. For example, candidates which raise little money and get little of the vote are probably in a situation where more money tangibly causes more votes because it can buy them the visibility which they so desperately need: however, they are unable to drive donations to the level which would buy them widespread visibility and are probably much lower quality candidates than incumbents and would face severe diminishing returns on increased spending. In contrast, incumbents seem to spend close to the maximum observed amount regardless of whether or not they are anticipating a strong challenger in the general election. Much of this can be explained by the fact that campaign donations cannot be saved so candidates are obligated to spend all the money they get. If incumbent fundraising can be thought of as either an effort to retain connections with key donors for potentially threatening races in the future or an effort by donors to buy favor with the likely winner, it makes sense that incumbent fundraising is both high and relatively inflexible in the face of large variations in the strength of competition. This analysis provides a different way to interpret the more obvious way of contextualizing a candidates spending: as a percentage of the total spending in that race (spending is adversarial so low amounts of spending isn’t necessarily indicative of an advertising advantage if the other candidate spends little as well). The relationship in the graph is linear and looks more clearly like a relationship one would expect if money caused candidates to win elections. However, one also observes a similar incumbent-challenger distribution over the spending measure to the previous graph. The relative invariance of incumbent spending suggests that this relationship is mostly a function of the relationship between challenger vote share and fundraising: this is hard to interpret as a causal effect of spending on vote outcomes. These facts motivate a restriction of the data to close elections (defined as elections where both candidates have a vote percentage within some bandwidth). There is a range of vote percentages around 50% within which incumbents and challengers seems to equally mixed. This is also the same range within which the maximum average amount of spending was observed over all different vote percentages. This analysis suggests that it is these elections where there are competent challengers who can fundraise and in which incumbents choose to juice their fundraising apparatus above what might be normally expected. Thus, we expect that marginal increases in spending are because candidates believe that spending will tangibly affect the vote which is the effect we want to identify. 4.3 Initial Regression Analysis The FEC reports a wide range of numbers about the fundraising for each candidates campaigns. Each election cycle (every 2 years) those candidates who ran for office are required to issue disclosure reports (in tandem with more granular reports on the way) regarding their fundraising, spending, cash on hand, etc which are in turn published. Variables are drawn from this dataset. A naive model would regress cash raised against vote shares with year and district fixed effects along with a control matrix to predict vote share. I’m going to run this regression and then explain why it is bad in lieu of a more robust regression based approach. The control variables are worth talking about because there are a lot of confounders in this dataset and multiple ways that we can at least approximate the confounders using the data available. This way we can obtain a regression that is at least mildly robust against many biases one could expect in a specification such as this. To start, one possibility is that higher candidate quality acts to dissuade primary competition. Since funding capacity for a candidate is probably mostly fixed, not needing to spend money in the primary could create an observed monetary and vote advantage for higher quality candidates through primary competition. One possible way to control for this is to add vote percentage during the primary as a control to the regression. However, given that primary competition should moderate the effect of increased fundraising, it makes more sense to interact the two terms in the regression. Unsurprisingly, general election and primary election shares are correlated, but not that strongly and whether that relationship persists after other variables are differenced out is a regression question. The general political beliefs of local populations also tends to remain relatively constant between election cycles. If there are systematic differences between election performance and fundraising between the parties, this might cause an erroneous positive coefficient. So, we use the performance in the previous election of the member of the same party to control for any party bias inherent to the district. Some parties also just perform better in certain years than others due to political circumstances that can’t be explicitly modeled. To try to estimate these, I include the aforementioned year-party fixed effects in the model. As mentioned earlier, perception of competition is an important determinant of candidate behavior. One way to possibly measure the candidates perception is the difference between what a candidates raises and what they spend. Those who leave a lot of money unspent are unlikely to feel they are facing a tough competitor. The same goes for donations from the party who are only likely to donate to elections they think they have a chance of swaying. Interacting these measures of competition with disbursements allows for a variable effect of disbursements depending on candidates perception of competition. Variables like party, incumbency status, and other relevant controls are also present as controls. The basic specification is as follows: \\[\\begin{equation} Vote\\%_{y,c} = Spending_{y,c} + FE_{d,y-1} + X_{y,c} + I_{y,c} + \\epsilon_{y,c} \\end{equation}\\] where I is the aforementioned interaction terms, y,c, and d are year, candidate, and district respectively, and epsilon is assumed to be i.i.d. gaussian. Regression results for the naive regression are reported below: Clearly there is a (very) significant positive coefficient between money a candidate has received and election share. However, as outlined above, this is in no way indicative of election success. Also worth noting is that this regression is generally terrible. There should be some regularization to enable easier feature selection and some variety of bootstrapping to avoid overfitting. But that would a bit overkill at this point so I’m going to do that in the next section. 4.4 IV Regression There are a few ways to identify a causal effect of money on election success. The cleanest way would perhaps be to look at state elections where one state has passed a fundraising law that differentially affects some candidates and not other (think raising some cap on union donations to PACs or a policy like that) which would allow us to do DnD between those two states. Unfortunately, state records tend to be spotty and I’m not aware of any such clean variation where data is also available. So, I resort to a next-best option: an IV regression. In this case, the instrument that I use is personal wealth. Candidates which are wealthier have more money and are able to donate to their own campaigns to increase their war chests in a way which is not indicative of public support. In fact, there seems to be little evidence that wealthy candidates do disproportionately well (besides the fact that basically all candidates have to be at least somewhat well off to take time off work to campaign in the first place). Instrument validity is relatively simple. The relevancy criteria is obviously met because more donations by the candidate directly lead to more spending capability by the campaign (this directness allows us to dodge the mental gymnastics that often characterize relevancy arguments). The main worry that we have to contend with in the exclusion requirement is that candidates who donate to themselves more also receive more donations from the public. A story might sound something like: candidates who can donate money to themselves use it as seed money to run advertisements which encourage more donations and create a virtuous cycle which isn’t available to self-funded candidates. Fortunately, there appears to be no relationship between different types of contributions: Running a regression between these two quantities also yields a negligible coefficient. The actual regression specification is similar to the previous one except TSLS is used where candidate donations are instrumented using candidate self-donations and the control vector. This regression is difficult to interpret as there are a ton of variables from the interaction terms. I tried a couple methods to keep model complexity low: namely LASSO and forwards-stepwise regression with bootstrapped error and an AIC selection criteria: however, none of these returned models with non-zero coefficients for the instrument, so I choose to report the whole model. There are a couple reasons this could be happening: 1) is that there is simply no relationship after the variable has been instrumented, or 2) the instrument simple isn’t powerful enough. Obviously we can’t rule out number 2, but the wide variety of candidates who donate to themselves comparable amounts as they receive in donations suggests that this effect isn’t important. Anyways, here’s the regression table for the instrumented regression: As can be seen, the instrumented coefficient is no longer significant providing evidence that the observed effect of money on vote share is most likely a result of OVB wrt candidate quality. 4.5 Discussion &amp; Conclusion The effect of money on politics is a topic which is highly controversial in our current democracy. The complaints often assume that money plays a determinative role in who gets elected. However, this is a claim that is particularly hard to marshall evidence for and this analysis would seem to suggest that, on the margin, more dollars, in and of themselves, don’t meaningfully contribute to the vote. Of course, this only measures marginal increases in fundraising in the house in certain years in competitive races and therefore could be subject to a variety of biases. The most concerning is that much money is funneled through a variety of PACs where spending can’t be measured due to a variety of reporting requirements and the level of this spending could be correlated with the instrument. This concerns are perhaps less severe than they would be for Senate or Presidential races as they get less funding and less attention than said races. However, it is still a real concerning source of bias. Nonetheless, the directness of the instrument and the fact that many donors almost double their individual contribution levels through donations means that the regression is strongly suggestive of a more complicated story about the effect of money. "],["neural-network-methods-for-electricity-demand-forecasting.html", "5 Neural Network Methods for Electricity Demand Forecasting 5.1 Introduction 5.2 Common Neural Network Designs for Forecasting 5.3 Problem Statement and Data 5.4 LSTM Results 5.5 CNN Results 5.6 Analysis", " 5 Neural Network Methods for Electricity Demand Forecasting 5.1 Introduction Electricity demand estimation is a classic problem in machine learning with many papers addressing potential approaches. The classical approach in the literature for time series estimation are CART methods where suitably deep trees can model the discontinuities induced by sudden changes in weather. However, this is an open problem that permits a variety of solutions such as classical time series models like exponential smoothing with a variety of seasonal adjustments or Monte Carlo simulations of weather models as the basis for ensemble learning. Not wanting to commit myself to simulating weather dynamics, I’m going to investigate time series forecasting of the local power demand in New England using several common neural network models for forecasting and compare their performance. 5.2 Common Neural Network Designs for Forecasting While there are an incredibly large class of models that can learn time dependencies (and innumerable additional variations on each broader class), there are only two main neural network architectures which are commonly recommended for general purpose time-series forecasting. The first are varieties of recurrent Neural Networks (RNNs) which learn how to create long term representations in memory of previous events. The most popular variants thereof are GRU networks and it’s more complicated, but more popular, cousin, the rather confusingly named long short-term memory network or LSTM. LSTMs blocks consist of a single layer which is unwound to learn a series of inputs in the manner of all similar RNNs. However, in addition to learning a weight matrix that maps into its latent space, LSTMs also learn a series of mappings into and out of a memory unit. This memory unit can be of an arbitrary dimensionality and the LSTM learns dependencies upon the memory unit which allow the content of the input to alter the memory unit (ie remember a current feature or forget a feature it has remembered in the past) or allows the memory unit’s content to alter current predictions. The LSTM learns these mappings in the usual RNN fashion of backpropogation through time though the number of number of parameters to learn is much higher per latent dimension due to the existence of the memory cell. These extra connections allows LSTMs to learn more complicated mappings back in time; functionally, LSTMs can be thought of as an extension on ResNet type architectures where LSTMs can learn more complicated types of skip connections. In fact, before transformers came into vogue for learning text embeddings, LSTMs were a popular option because of their ability to learn long term dependencies between different types of words in a sentence. It is this same property to learn longer term dependencies that we wish to exploit for short term power forecasting. The second primary method for learning time dependencies are convolution neural networks (CNNs). CNNs originated from adaptations of filtering techniques in image processing to neural networks; as a result, CNNs are primarily popular for their applications to image classification tasks. The traditional CNN design alternates filter layers where an arbitrary number of discrete filters from classical image processing (think sharpening or blurring filters) are convolved with different color layers of an image to produce feature mappings. These features are then fed into pooling layers which are essentially dimensionality reduction techniques that attempt to preserve spatial relationships between features. By chaining these layers together, one can build feature representations that are roughly translation invariant which is good for image classification where translation invariance is an important property. These same techniques can also by applied to applied to time series analysis by reducing the input dimension by one. Whereas in image classification 2d filters are applied across different color channels, in time series 1d filter are applied across different input features while pooling and fully connected layers are identical. While translation invariance and pooling layers are less important here, the local nature of filter layers allow the network to learn to identify types of events that have heterogenous future effects on the series and then propogate those forwards in the fully connected layers. 5.3 Problem Statement and Data Short term electricity demand forecasting is an important task for managerial authorities who control the transmission of electricity and must respond to changing consumer demand and generation authorities who must make decision about whether it is profitable to turn plants on or off at certain times. Having robust models for all these parties allows operators to make more informed decisions about aligning supply with demand and thus creates more efficient market outcomes while avoiding problems like rolling blackouts or brownouts due to underprovision. We attempt to forecast per state demand in New England using previous values of demand in the relevant state and in neighboring states. Given that demand is highly dependent on temperature and thus time of year, the actual timestamps on the time series can also be used as features. In this case, we transform the month and hour components using sin-cosine transforms and use those as feature inputs. Our data is from the Energy Information Administration which, unsurprisingly, publishes energy demand information. Specifically, the EIA publishes per hour demand information broken down by specific state, grid authority, and region. We use the EIA API to pull a three year stretch of demand information over a group of New England states which we then plug into our neural network designs. I train on short term prediction accuracy, ie how good is the network at predicting the next hour of demand. This is a relatively approachable task that demands the network learn both trends over time, level changes over seasons, and short terms changes across days (ie over 24 or so samples). 5.4 LSTM Results The main parameter to tune in LSTMs is the number of latent features. This allows the network to learn more complex feature representations across time. However, the number of parameters that arise from an increase in feature dimension is large meaning such an increase must be used sparingly if training time is an issue. Take this Keras printout from a relatively simple network of an LSTM layer followed by a fully connected layer which predicts one output: ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## lstm (LSTM) (1, 20) 2480 ## dense_1 (Dense) (1, 20) 420 ## dense (Dense) (1, 1) 21 ## ================================================================================ ## Total params: 2,921 ## Trainable params: 2,921 ## Non-trainable params: 0 ## ________________________________________________________________________________ The number of parameters are 5x greater for the LSTM layer despite having the exact same input and output dimension as the fully connected layer; this fact motivates designs that include fewer LSTM layers the number/dimensions of LSTM layers. In this instance, I will only be using 1 LSTM layer. Hierarchical LSTMs which use chained LSTMs to generate results are popular in some applications but are not strictly necessary to create sufficient model complexity due to the fact that LSTMs are recurrent and effectively generate depth by unwinding through time. The base model I use is an LSTM layer followed by a batch normalization and two fully connected layers with weight dropout as regularization. I first compare the efficacy of different LSTMs with for different hidden dimensions. Theoretically, more dimensions may allow the LSTM to model more complicated time trends: however, greater dimension also makes it substantially harder to train and prone to overfitting the test data more rapidly. We test the same data with LSTMs of dimension 8, 32, and 128, with mixed results. As can be seen below, the two smaller models perform comparably on the test data while the larger model starts to overfit around 25 epochs and is substantially more volatile. The most probable reason for this is a learning rate that is too large (.01 in this case). One of the nice results that comes out of the literature equating SGD updates to stochastic differential equation flows is that lower learning rates lead to greater approximations to the ideal SDE and thus lesser instability than what we see while more parameters pushes in the opposite direction (Jastrzębski et al 2018 is a great paper on this). I would say that it’s highly probable the 128 unit model would perform at a lower learning rate, but I don’t have the heart (or the time) to put my poor laptop through a grid search of said model, so I’m going to be looking at the 32 dimension model. Given that we only want to compare performance to CNNs and I’m definitely not going to be doing extensive hyperparameter tuning there (the Keras function has like 20 arguments), I’d say this permissible. That said, learning rate is probably going to be the other main parameter beside model size that determines how well the model fits, so we’re going to check a few values for the 32 unit model to see how well the model generalizes. Given that the learning rate was probably too high for the 32 unit model judging by the slight instability in the training data, we try two smaller values. It is clear that a learning rate of .001 wins out in this case and it seems that we hit it more or less on the head as both other learning rates exceed the test loss. Notably, for the blue .001 loss, the MSE has a valley at around 25 epochs and spikes upwards. We can say that playing around with higher epoch counts, early stopping is highly beneficial in this context as it has the capacity to overfit rather strongly otherwise. Note that the loss is MSE rather than MAE and the data is standardized which means that average error is around .3 standard deviations for the optimal model, which, all in all, isn’t terrible and serves as a reasonable benchmark with which to compare CNN architectures. 5.5 CNN Results Remember when I said that I wasn’t going to do grid search on CNN parameters? I meant that. I could spend all my day (and computing power) sitting around playing with stride, activation functions, pooling layer width, kernel size, padding policy, etc. I’m not going to do that because it sounds boring. Also, that not quite the point here when I only want to get a rough comparison between LSTMs and CNNs. The thing about CNNs is that they don’t have complicated state transition maps which means that they don’t require comparable parameters to a LSTM for the same depth. The first thing we test for CNNs is also model complexity and to make things fair we’ve decided to create architectures with comparable parameters counts. Note that counting parameters isn’t a perfect solution here - models take different amounts of time to train and offer different amounts of flexibility per parameter. If increased parameter count concerns us because of increased training time, trying to measure said training time and equate it between models creates a very difficult situation where results are highly heterogenous between computer systems with different memory architectures despite identical algorithms (on top of the obvious semantic problems). If it concerns us because we want to equate model expressivity, the explicit bounds in the literature are both loose and very narrow while trying to measure effective degrees of freedom through some sort of matrix method (think smoothing splines) requires us to train models and identify global minima: which we can’t do. So, we just take the naive approach and assume that equality of parameters roughly implies equality of other quantities of interest. The structure of our CNN model is relatively simple. Two convolution layers with kernels of size 19 and 11 are interspersed with 2 max pooling layers with kernels of size 3, which is followed by a fully connected layer that is linked to the output. The number of filters for each of the convolution layers is varied in order to achieve rough equality in the number of parameters between the comparable LSTM models and the CNNs. After specifying the model, we run into a big issue that isn’t present in LSTMs: the decision about how many lags to include when training the CNN. Unlike LSTMs where past dependencies are only limited by how long your data goes back, CNNs require that all lags where one wishes to have dependencies be fed in at time of prediction. In some ways this is beneficial as it allows greater parralellism and thus greater batch sizes (whether this is desirable is debatable) that speed up training. The flip side is that the memory costs are much higher for CNNs, especially at higher batch sizes as the input requirements mean data duplication is almost always necessary. We had the frustrating experience of getting a memory overflow error a couple of time during training what is a relatively small dataset for this project (admittedly without a GPU or much RAM, but still). This consideration is probably going to be one that will dominate your thinking when choosing a specific model rather than efficiency concerns and the memory restrictions in our case means we can only model dependencies for a lag of up to 120 samples. A 120 sample restriction means it will be harder to model long term dependencies: let’s see if that show up in the loss. The performance is not very good; especially for the larger model which finds itself a nice local minima in the training data (despite all my prodding to make it not do that) and overfits almost immediately on the test data. The medium size model performs a bit less well on the training data but it seems like it might generalize slightly better even if it’s not overparametrized so I’ll use that to test different learning rates on. ## Warning: Removed 1 row containing missing values (`geom_line()`). ## Removed 1 row containing missing values (`geom_line()`). The lower learning rates are comparable to each other while the higher one clearly struggles and bounces around in the training data. For the test data, they all suck. The optimal LSTM model had a test error (with early stopping) of around 1/2. The best CNN is 50% greater than that. Perhaps this is a problem that could be corrected with enough training but there’s no a priori reason to believe an LSTM wouldn’t also improve a commensurate amount. 5.6 Analysis This analysis seems to indicate that LSTMs outperform CNNs on this specific dataset. This fact is not terribly surprising. We’ve already outlined how data limitations on CNNs make them less useful when modeling longer term dependencies. In addition, the kernel-pooling structure of typical CNNs is made to create translation invariance in image classification tasks which is not what one necessarily wants in a time-series regression context. If there’s one good lesson here it’s probably that CNNs really aren’t as good as LSTMs for this specific sort of task but that one should really let one’s problem domain guide model choice. Technically, CNNs and LSTMs are both models that can function in a time-series context. However, when one needs to model longer term dependencies (like fluctuations in power demand over the week when data is this granular), CNNs have specific constraints that make them difficult to use from an efficiency and memory perspective. However, I suspect that if one only needed to model short term dependencies, CNNs would be more efficient as they more explicitly model connections. And of course, this is only one experiment on the two most common sorts of models that are recommended for these sorts of tasks. One could also use an explicit time-series model (ARMA or the like), a random forest, smoothing splines, or even the combined CNN-LSTM models which are often held up as more efficient than either one. All that said, there are reasons why people don’t readily apply certain types of models to data like this and if you choose to use a hyper-flexible model for time series, it should be because there are a multitude of factors that weigh in favor of its use. "],["the-effect-of-seed-market-competition-on-farmers.html", "6 The Effect of Seed Market Competition on Farmers", " 6 The Effect of Seed Market Competition on Farmers This is my bachelor’s thesis in economics. I don’t have the original Latex or figures right now to recompile the thing here, so I’ve just embedded the pdf for you viewing edification. "],["chicago-taxi-demand-estimation.html", "7 Chicago Taxi Demand Estimation 7.1 Introduction 7.2 Full Data Procedures", " 7 Chicago Taxi Demand Estimation 7.1 Introduction Demand estimation is an important topic for businesses of any kind: accurate demand forecasts can allow them to operate on higher margins while poor forecasts risk either disappointing clients due to a lack of supply or saddling the business with excessive or labor without a buyer for the product. While the input for the taxi market is just labor, the analysis is no different. Fortunately, the City of Chicago has a dataset where they release data on the time rounded to the nearest quarter hour, location, and duration of literally every taxi ride in the city (or at least that’s what they say). This dataset allows us to do fairly robust demand estimation due to its granularity. The most important decision for a taxi decision for a taxi firm is a) when to hire more drivers, and b) when to schedule more drivers. To this end, I will be doing both seasonal and daily trend predictions. Unfortunately, observed quantity is endogenous. Attempts to supply more taxis within the data period will increase observed quantity (which I might erroneously interpret as increased demand): fortunatley, the data allows reparametrizations that decrease these concerns which I will explore at the end. 7.1.1 Data The dataset provided by the City of Chicago is quite robust in that it appears to be quite clean already. The problem is that each row is an individual ride and the timespan of the dataset is 2002 to present. Chicago is a big city with a lot of people and a substantial number of taxis. The resulting dataset has over 200 million observations. My computer is puny and even SQL queries to a local server are very slow. So, I’ll be first analyzing 2022 data as an outline of our methods and approaches and then extend this analysis and restimate models with multiple years later. The dataset also allows multiple ways to estimate demand. Ride data comes with length, time, and fare data which might allow us to create a more accurate picture of demand. For the first part, we will use count data, but then diversify to other types of demand measures later. 7.1.2 Trends There is multiple levels on which one can examine demand in this dataset. The most trivial would be to examine raw number of rides within a timespan. There are already some interesting trends in this data at first glance. For one, there is a solid trend upwards through most of the study period until around November at which point the demand seems to dip back down. Chicago gets very cold in winters and it makes sense that demand would be negatively affected during that period. However, demand does not seem to achieve the lows that it did at the start of the previous year which would suggest long term growth in the demand for taxis. From a technical perspective, the demand is definitely not stationary and is looking like it might not even be first-difference stationary. On a slightly different topic, the variance of the measurements increase as the year goes on which has two separate implications: OLS isn’t going to be enough here. If we wanted to use regression we would need something like a quasi-likelihood model that controls for heteroskedasicity in order to get meaningful confidence intervals. Secondly, if we want to use some sort of exponential smoothing model, the increased variance suggests that the right approach is to use one with multiplicative trends as opposed to additive ones (more on this later). The next important question is what kind of seasonality is there in this data. We’ll address year seasonality later, but right now we can visualize daily and weekly seasonality already present in the data. The previous graph shows a 100 tap moving average filter that is fitted to demand measurements at the lowest granualarity: counts of rides numbers in 15 minute bins supplied in the dataset. The distinct peaks that we can see correspond to weeklong periods. Plotting average number of rides by weekday gives the following graph: If you look on the y axis, the differences between low volume days and high volume days is only 10% or so, but this is still enough to create a noticeable seasonal trend that shows up in the weighted average. There are also clear seasonal trends across days in the day that look roughly similar to a sine wave. We can group data by time of day (without controlling for anything else) and get the following visualization: This isn’t quite sinusoidal, but it’s close, and it’s about what we would expect if demand was a smooth function of time of day. Note that, once again linear regression isn’t an appropriate choice here. Even polynomial regression is undesirable as the Taylor series of a sine wave converges rather slowly on the boundary of a compact interval centered on the point of expansion compared to the center of the set which results in more parameters than necessary. Not to spoil the fun, but there are going to be seasonal yearly trends in this dataset too which complicate our ability to apply simple estimation procedures. 7.1.3 Possible Estimation Procedures Not to beat a dead horse here, but our data is not stationary. Applying a KPSS test with a null hypothesis that the data is trend stationary gives a significant p-value. One of the primary reasons for this seems to be the seasonality which we have a variety of ways to deal with. ## ## KPSS Test for Trend Stationarity ## ## data: as.numeric(daily_rides_2022$num_rides) ## KPSS Trend = 0.29748, Truncation lag parameter = 16, p-value = 0.01 The basic approach to seasonal time-series modeling is to use some sort of moderately flexible exponential-smoothing/FIR model that has the ability to remove a seasonality or trend component specified by the researcher. SARIMA, Holt-Winters, and periodogram based approaches are all examples. The problem with all the previous approaches is that they only allow the specification of one seasonal component. Obviously, one could estimate seasonal components separately through use of some sort of restricted AR model and then use a standard estimation technique on the residuals. However, this approach is undesirable as having models with multiple levels are hard to debug and allow model error to more easily be propagated through residuals as opposed to estimating all the parameters simultaneously. This means a more flexible model is needed in order to fit. One upside of our data is that we can be pretty sure that season periods are fixed. This means that flexible season length methods like STL aren’t strictly necessary, but it actually ends up to work pretty well here so that’s what we’re going to use. The main challenge in working with this data is the nonconstant variance structure which limits the effectiveness of highly parametric methods and pushes us towards more flexible local methods. There are a few possible ways to deal with this issue. The easiest is to transform the data to create a more desirable variance structure: the most common transformation is obviously the logarithm where the compressive nature of the log pushes variances closer together (this also has the benefit of turning multiplicative relationships into additive ones). Taking the log actually works pretty well here and generates data that looks like it has normal errors but doesn’t necessarily mean that Holt-Winters performs well. Another possible approach is to model the variance explicitly: the easiest method would be fitting some sort of quasi-likelihood model to the time series. However, we find this approach generally suspicious unless one has a prior that the variance conforms to a set structure. There also isn’t much literature on this approach and I don’t want to code a package that implements something like that…so there you go. Luckily, the dual approach to this is to use a method that is flexible enough to model the variance itself which is well studied and has a few good algorithms for time-series. We’ve already spoiled that we’re choosing the third option and using STL to perform the trend/seasonality decomposition. So, let’s get to it. 7.1.4 STL Decomposition One challenge with forecasting in the smaller context we’re operating in right now is that we don’t know if the dip down at the end of the period is a yearly seasonal effect or just an exogenous shock. There’s not a ton to do about this right now, but just be aware there’s some grains of salt to be had until we move to the full analysis. As is easily apparent, the STL decomposition for the trend is pretty gritty, but that’s a benefit insomuch as it allows us to get a stable estimate of the seasonal component that changes relatively little over the course of the study period. Basically all modern time series procedures involve a decomposition of a series into trend-cycle, seasonal, and irregular elements and the estimation of those elements individually. STL gives me that decomposition that I can then play with in order to get a good approximation of the next few periods. The strategy is pretty simple. The last few samples of the seasonal decomposition give me an estimate of the trend-cycle at that point (this can be estimated using a constant or interpolating polynomials (I use interpolating loosely here), where I choose the latter). Seasonality can be estimated simply repeating the next few samples of what it would have been from the previous season and the irregularities can be estimated using a feedforward ARIMA design. A nice benefit of the ARIMA approach to estimating irregularities is that it preserves variance without transforming the data thus aiding in interprebility. Here I present a ten day forecast for demand. You can also theoretically do the same procedure for hourly demand with a less flexible LOESS selection and day seasonality on top of week seasonality. Note that the confidence intervals are only the sum of the natural intervals from the ARIMA model and the prediction intervals from the interpolating polynomial (note that the constant residual variance assumption for the prediction intervals is approximately satisfied since we’re only pulling from a rough period near the end of the series). Note that this specification doesn’t guard against model error and the projection would have done a very poor job of predicting the downturn if they had been taken a few weeks prior. In that sense, the confidence intervals are very optimistic (especially considering that we will see there is yearly seasonality that hasn’t been accounted for). However, they do provide an easy an decomposable heuristic for prediction which we will undertake in the next section with the whole data. 7.2 Full Data Procedures The full data contains all taxi rides from 2013 to 2022 in Chicago and allows us to decompose year trends from trend-cycle data. A few things are notable about the full data. For one, yearly seasonality is easy to observe in this context where demand peaks during the middle of the year and tapers off in the colder months (when it is perhaps understandable that people have less desire to be outside in the Chicago winter). Another important note is that the start of the coronavirus pandemic is easy to observe near the beginning of 2020 and has a clear effect on both the level and the variance of the data. Longer term trends in demand are also present in the data. Demand seems to rise between 2013 and 2015 while declining before the pandemic (perhaps due to greater competition from rideshare services or due to demographic trends in Chicago itself). It is also easy to observe a level effect on the variance that is most apparent in the summer months and post-Covid. Given the our trend-cycle estimates come from STL which is a fairly flexible method, we really don’t need to modify the previous procedure except to add seasonality (besides that fact that I’m going to be waiting around a lot longer for my computer to plod through all the data). So, let’s do that. Training is essentially identical and we get an STL decomposition, a trained ARIMA model, and a fitting procedure after all is said and done. Let’s see how the model forecasts the last week of the data compared to the actual data: As can be seen, the estimation method tracks relatively well with the actual data. Obviously, much of this is owed to the relatively stable seasonality and the existence of a non-stochastic trend element. Nonetheless, this method is both easy to implement in many common statistical packages and is relatively easily interpreble given that there is an explicit decomposition in the method between the seasonal, trend-cycle, and irregular elements. This makes it easy to use and to extend. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
